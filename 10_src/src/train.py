"""
Vesuvius Challenge 2026 - 3D æ¨¡å‹è®­ç»ƒè„šæœ¬ (å‡çº§ç‰ˆ + åŠ¨æ€ç­–ç•¥)
æ”¯æŒ AMP è®­ç»ƒã€å®éªŒç®¡ç†ã€å®æ—¶ç›‘æ§ã€è‡ªåŠ¨ç»˜å›¾å’ŒåŠ¨æ€ GPU ç­–ç•¥
"""

import os
import sys
import argparse
import time
from pathlib import Path

import torch
import torch.optim as optim
# [FIX] ä½¿ç”¨æ–°ç‰ˆ scalerï¼Œæˆ–è€…ç»§ç»­ä½¿ç”¨ GradScaler (å®ƒé€šå¸¸å…¼å®¹)
# torch.amp.GradScaler is available in newer versions, but torch.cuda.amp.GradScaler is alias.
# We will use torch.cuda.amp.GradScaler for compatibility but autocast will be updated.
from torch.cuda.amp import GradScaler
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.dataset import create_dataloader, VesuviusDataset
from src.model import ResUNet3D, ResUNet3DWithAffinity, compute_affinity_target, MAMBA_AVAILABLE
from src.loss import CombinedLoss, CombinedLossWithAffinity
from src.surface_loss import CompoundLoss
from src.utils import ExperimentManager, plot_training_curves, plot_multi_patch_comparison, plot_3d_comparison, format_time, DynamicGPUManager, generate_high_res_sample




def get_args():
    parser = argparse.ArgumentParser(description='Train 3D ResU-Net for Vesuvius Challenge')
    
    # è·¯å¾„å‚æ•°
    parser.add_argument('--data_dir', type=str, default='data/vesuvius-challenge-surface-detection', help='æ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='10_src/outputs', help='å®éªŒè¾“å‡ºæ ¹ç›®å½•')
    parser.add_argument('--exp_name', type=str, default='ResUNet', help='å®éªŒåç§°å‰ç¼€')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=6, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--accumulation_steps', type=int, default=1, help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    parser.add_argument('--lr', type=float, default=3e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='æƒé‡è¡°å‡')
    
    # [FIX] å¼ºåˆ¶ä½¿ç”¨æ­£æ–¹ä½“ Patch å°ºå¯¸ (D, H, W) = (64, 64, 64)
    # è¿™ç¡®ä¿äº†è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯æ­£æ–¹ä½“ï¼Œç¬¦åˆ TIF åŸç”Ÿç»“æ„ï¼Œä¸”å¯è§†åŒ–ç»“æœä¹Ÿæ˜¯æ­£æ–¹ä½“
    parser.add_argument('--patch_size', type=int, nargs=3, default=[64, 64, 64], help='è®­ç»ƒ Patch å°ºå¯¸ (D, H, W)')
    parser.add_argument('--num_workers', type=int, default=4, help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    
    # Loss å‚æ•°
    parser.add_argument('--alpha', type=float, default=0.3, help='BCE Loss æƒé‡')
    parser.add_argument('--beta', type=float, default=0.5, help='Tversky Loss æƒé‡')
    parser.add_argument('--gamma', type=float, default=0.15, help='clDice Loss æƒé‡')
    parser.add_argument('--dynamic_beta', type=float, default=1.0, help='åŠ¨æ€ BCE æƒé‡å› å­')
    parser.add_argument('--tversky_alpha', type=float, default=0.3, help='Tversky FP æƒé‡ (æƒ©ç½šåšåº¦/å‡é˜³æ€§)')
    parser.add_argument('--tversky_beta', type=float, default=0.7, help='Tversky FN æƒé‡ (æƒ©ç½šå¬å›/å‡é˜´æ€§)')
    parser.add_argument('--cldice_warmup', type=int, default=5, help='clDice é¢„çƒ­è½®æ•° (æ­¤æœŸé—´ gamma=0)')
    
    # Affinity æ‹“æ‰‘æ„ŸçŸ¥åˆ†æ”¯
    parser.add_argument('--use_affinity', action='store_true', help='å¯ç”¨ Affinity æ‹“æ‰‘æ„ŸçŸ¥åˆ†æ”¯ + Mamba å…¨å±€ä¸Šä¸‹æ–‡')
    parser.add_argument('--affinity_weight', type=float, default=0.0, help='Affinity Loss æƒé‡')
    parser.add_argument('--dilation_iters', type=int, default=1, help='è®­ç»ƒé›†æ ‡ç­¾è†¨èƒ€è¿­ä»£æ¬¡æ•° (0=å…³é—­, 1-2=å»ºè®®å€¼)')
    
    # CompoundLoss (è¡¨é¢æ„ŸçŸ¥æŸå¤±)
    parser.add_argument('--use_compound', action='store_true', help='ä½¿ç”¨ CompoundLoss (BCE + SurfaceDice + Boundary) æ›¿ä»£ CombinedLoss')
    parser.add_argument('--w_bce', type=float, default=1.0, help='CompoundLoss: BCE æƒé‡')
    parser.add_argument('--w_surface', type=float, default=1.0, help='CompoundLoss: SurfaceDice æƒé‡')
    parser.add_argument('--w_boundary', type=float, default=0.5, help='CompoundLoss: Boundary æƒé‡')
    parser.add_argument('--tau', type=float, default=2.0, help='CompoundLoss: SurfaceDice å®¹å·®åŠå¾„ (ä½“ç´ )')
    parser.add_argument('--boundary_warmup', type=int, default=5, help='CompoundLoss: Boundary Loss å¼€å§‹ç”Ÿæ•ˆçš„ Epoch')
    
    # æ–­ç‚¹ç»­è®­
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼ (åªä½¿ç”¨å°‘é‡æ•°æ®)')
    parser.add_argument('--resume', type=str, default=None, help='æ–­ç‚¹ç»­è®­: checkpoint è·¯å¾„')
    parser.add_argument('--reset_optimizer', action='store_true', help='é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€ (ä¸æ–°æŸå¤±å‡½æ•°é…åˆä½¿ç”¨)')
    
    return parser.parse_args()


def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)


def gpu_dilation(mask, iterations=1):
    """
    ä½¿ç”¨ GPU ä¸Šçš„ 3D MaxPool æ¨¡æ‹Ÿå½¢æ€å­¦è†¨èƒ€
    æ¯” CPU ä¸Šçš„ scipy.ndimage.binary_dilation å¿« 10-100 å€
    """
    if iterations <= 0: return mask
    for _ in range(iterations):
        mask = torch.nn.functional.max_pool3d(mask, kernel_size=3, stride=1, padding=1)
    return mask


def train_epoch(model, loader, optimizer, criterion, scaler, device, accumulation_steps=1, epoch=1, args=None):
    model.train()
    running_loss = 0.0
    running_bce = 0.0
    running_dice = 0.0
    running_cldice = 0.0
    running_aff_loss = 0.0
    
    use_affinity = args.use_affinity
    
    # åŠ¨æ€è°ƒæ•´ gamma / epoch (åŒºåˆ† CompoundLoss å’Œ CombinedLoss)
    use_compound = getattr(args, 'use_compound', False)
    if use_compound:
        # CompoundLoss: è®¾ç½® epoch ç”¨äº boundary warmup
        criterion.set_epoch(epoch)
    else:
        active_criterion = criterion.seg_loss if use_affinity else criterion
        if epoch <= args.cldice_warmup:
            active_criterion.gamma = 0.0
        else:
            active_criterion.gamma = args.gamma
        
    optimizer.zero_grad()
    
    # æ„å»ºè¿›åº¦æ¡æè¿°
    current_lr = optimizer.param_groups[0]['lr']
    if use_compound:
        desc_info = f"Train E{epoch} (LR={current_lr:.6f})"
    else:
        gamma_val = active_criterion.gamma
        desc_info = f"Train E{epoch} (Gamma={gamma_val}, LR={current_lr:.6f})"
    
    # [FIX] ä½¿ç”¨ dynamic_ncols=True å’Œ ascii=False å°è¯•ä¿®å¤æ¢è¡Œé—®é¢˜
    # å¦‚æœ VSCode ç»ˆç«¯ä»æœ‰é—®é¢˜ï¼Œå¯ä»¥å°è¯• ascii=True
    pbar = tqdm(loader, desc=desc_info, leave=True, dynamic_ncols=True, mininterval=0.5)
    
    for i, (images, labels) in enumerate(pbar):
        images = images.to(device)
        labels = labels.to(device)
        
        # [P0 FIX] å¤„ç† 2-channel Labels (Ch0: Label, Ch1: ValidMask)
        if labels.shape[1] == 2:
            valid_mask = labels[:, 1:2, ...]
            labels = labels[:, 0:1, ...]
        else:
            # Fallback for old data or if dataset not updated (safety)
            valid_mask = torch.ones_like(labels)
            
        # [GPU åŠ é€Ÿ] åœ¨ GPU ä¸Šè¿›è¡Œæ ‡ç­¾è†¨èƒ€ (ä»…è®­ç»ƒæ—¶)
        # æ³¨æ„ï¼šä»…è†¨èƒ€ labelï¼Œä¸è†¨èƒ€ valid_mask (ä¿æŒåŸå§‹æœ‰æ•ˆåŒºåŸŸ)
        if args.dilation_iters > 0:
            with torch.no_grad():
                labels = gpu_dilation(labels, iterations=args.dilation_iters)
        
        # [Sanity Check] é¦– epoch é¦– batch æ£€æŸ¥æ ‡ç­¾æœ‰æ•ˆæ€§ (ç¨€ç–æ€§éªŒè¯)
        if epoch == 1 and i == 0:
            label_sum = labels.float().sum().item()
            label_total = labels.numel()
            label_ratio = label_sum / label_total
            label_max = labels.max().item()
            
            print(f"\n[Sanity Check] Batch 0 Label Stats:")
            print(f"  Max Value: {label_max}")
            print(f"  Positive Ratio: {label_ratio*100:.2f}% (Should be < 10%, Warning if > 30%)")
            
            if label_max == 0:
                print("âš ï¸ WARNING: Label is all black (0). Check dataset path or IDs!")
            if label_ratio > 0.30:
                print(f"âš ï¸ CRTICAL WARNING: Label sparsity is suspect ({label_ratio*100:.1f}%)!")
                print("  POSSIBLE CAUSE: 'Ignore' regions (val=2) are being treated as targets.")
                print("  ACTION: Check src/dataset.py label binarization logic.")
        
        # å‰å‘ä¼ æ’­ (è‡ªåŠ¨åŒºåˆ† Affinity / æ ‡å‡†æ¨¡å¼)
        with torch.amp.autocast('cuda'):
            if use_affinity:
                seg_logits, aff_logits = model(images)
                aff_targets = compute_affinity_target(labels)
                # ä¼ é€’ valid_mask ç»™ Loss
                loss, bce, dice, cldice, aff_loss = criterion(seg_logits, aff_logits, labels, aff_targets, valid_mask=valid_mask)
            else:
                outputs = model(images)
                # CompoundLoss å’Œ CombinedLoss éƒ½è¿”å› 4 å…ƒç»„
                # ä¼ é€’ valid_mask ç»™ Loss
                loss, bce, dice, cldice = criterion(outputs, labels, valid_mask=valid_mask)
            loss = loss / accumulation_steps
            
        # æ··åˆç²¾åº¦åå‘ä¼ æ’­
        scaler.scale(loss).backward()
        
        if (i + 1) % accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
        
        # è®°å½•æ•°æ®
        running_loss += loss.item() * accumulation_steps
        running_bce += bce.item()
        running_dice += dice.item()
        running_cldice += cldice.item()
        
        # [DEBUG] æ¯ 50 ä¸ª batch æ‰“å°åƒç´ çº§ç»Ÿè®¡ (å¯¹æ ‡ 20_src)
        if (i + 1) % 50 == 0:
            with torch.no_grad():
                if use_affinity:
                    pred_prob = torch.sigmoid(seg_logits)
                else:
                    pred_prob = torch.sigmoid(outputs)
                
                pred_pixels = (pred_prob > 0.5).float().sum().item()
                gt_pixels = labels.sum().item()
                mean_prob = pred_prob.mean().item()
                
                # ä½¿ç”¨ \r è¦†ç›–å½“å‰è¡Œæˆ–æ¢è¡Œ
                tqdm.write(f"\n[DEBUG] Batch {i+1}: Pred_Pixels={int(pred_pixels)}, GT_Pixels={int(gt_pixels)}, Mean_Prob={mean_prob:.4f}")

        # æ›´æ–° TQDM Postfix (æ˜¾ç¤º Dice Score è€Œé Loss)
        current_lr = optimizer.param_groups[0]['lr']
        # æ³¨æ„: criterion è¿”å›çš„æ˜¯ dice loss (1-dice)
        # å³ä½¿ä½¿ç”¨äº† Tversky, ä¹Ÿå¯ä»¥è¿‘ä¼¼æ˜¾ç¤º 1-loss ä½œä¸º soft score
        soft_dice_score = 1.0 - dice.item()
        
        pbar.set_postfix({
            'loss': f"{loss.item() * accumulation_steps:.3f}",
            'dice': f"{soft_dice_score:.3f}", # Training Dice (Score, higher is better)
            'lr': f"{current_lr:.1e}",
        })
        
    avg_loss = running_loss / len(loader)
    avg_bce = running_bce / len(loader)
    avg_dice = running_dice / len(loader) # Loss
    avg_cldice = running_cldice / len(loader)
    
    return avg_loss, avg_bce, avg_dice, avg_cldice


@torch.no_grad()
def validate(model, loader, criterion, device, use_affinity=False):
    model.eval()
    total_loss = 0
    total_bce = 0
    total_dice_loss = 0
    total_cldice = 0
    total_val_dice_score = 0
    
    # ç”¨äºå¯è§†åŒ–çš„æ ·æœ¬åˆ—è¡¨ (æ•è·å‰ 4 ä¸ª batch çš„ç¬¬ä¸€ä¸ªæ ·æœ¬)
    vis_samples = []
    
    # ç”¨äºå¯è§†åŒ–çš„æ ·æœ¬åˆ—è¡¨ (æ•è·å‰ 4 ä¸ª batch çš„ç¬¬ä¸€ä¸ªæ ·æœ¬)
    vis_samples = []
    
    pbar = tqdm(loader, desc="Validating", leave=False, dynamic_ncols=True, mininterval=0.5)
    
    for i, (images, labels) in enumerate(pbar):
        images = images.to(device)
        labels = labels.to(device)
        
        # [P0 FIX] å¤„ç† 2-channel Labels
        if labels.shape[1] == 2:
            valid_mask = labels[:, 1:2, ...]
            labels = labels[:, 0:1, ...]
        else:
            valid_mask = torch.ones_like(labels)
            
        with torch.amp.autocast('cuda'):
            if use_affinity:
                seg_logits, aff_logits = model(images)
                aff_targets = compute_affinity_target(labels)
                loss, bce, dice, cldice, aff_loss = criterion(seg_logits, aff_logits, labels, aff_targets, valid_mask=valid_mask)
                outputs = seg_logits  # åç»­å¯è§†åŒ–å’Œ Dice è®¡ç®—ä½¿ç”¨åˆ†å‰²å¤´è¾“å‡º
            else:
                outputs = model(images)
                loss, bce, dice, cldice = criterion(outputs, labels, valid_mask=valid_mask)
        
        # æ•è·å‰ 4 ä¸ª batch çš„æ ·æœ¬ç”¨äºå¯è§†åŒ–
        if len(vis_samples) < 4:
            probs_vis = torch.sigmoid(outputs)
            vis_samples.append({
                'raw': images[0, 0].cpu().numpy(),  # (D, H, W)
                'gt': labels[0, 0].cpu().numpy(),   # (D, H, W)
                'pred': probs_vis[0, 0].cpu().numpy()  # (D, H, W)
            })
            
        total_loss += loss.item()
        total_bce += bce.item()
        total_dice_loss += dice.item()
        total_cldice += cldice.item()
        
        # è®¡ç®— Dice Score (é˜ˆå€¼ 0.5)
        probs = torch.sigmoid(outputs)
        preds = (probs > 0.5).float()
        
        intersection = (preds * labels).sum()
        union = preds.sum() + labels.sum()
        dice_score = (2. * intersection + 1e-6) / (union + 1e-6)
        total_val_dice_score += dice_score.item()
        
        # [Output] å®æ—¶æ›´æ–°éªŒè¯è¿›åº¦
        pbar.set_postfix({
            'loss': f"{total_loss / (i+1):.4f}",
            'dice': f"{total_val_dice_score / (i+1):.4f}"
        })
        
    avg_loss = total_loss / len(loader)
    avg_bce = total_bce / len(loader)
    avg_dice = total_dice_loss / len(loader)
    avg_cldice = total_cldice / len(loader)
    avg_val_dice_score = total_val_dice_score / len(loader)
    
    return avg_loss, avg_bce, avg_dice, avg_cldice, avg_val_dice_score, vis_samples


def main():
    # [CRITICAL] å¼ºåˆ¶æ£€æŸ¥ GPU
    assert torch.cuda.is_available(), "CRITICAL: No GPU found! Check CUDA installation."
    
    args = get_args()
    set_seed(args.seed)
    
    # 1. å®éªŒåˆå§‹åŒ–
    exp = ExperimentManager(args, experiment_name=args.exp_name)
    
    # 2. GPU ç®¡ç†å™¨åˆå§‹åŒ–
    gpu_manager = DynamicGPUManager()
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda')
    print(f"Using device: {device}")
    
    # è·¯å¾„é…ç½®
    csv_path = os.path.join(args.data_dir, 'train.csv')
    image_root = os.path.join(args.data_dir, 'train_images')
    label_root = os.path.join(args.data_dir, 'train_labels')
    
    # æ•°æ®é›†å‡†å¤‡ (è®­ç»ƒé›†å¸¦æ ‡ç­¾è†¨èƒ€ï¼ŒéªŒè¯é›†ä¸å¸¦)
    print("Initializing dataset...")
    full_dataset = VesuviusDataset(
        csv_path=csv_path,
        image_root=image_root,
        label_root=label_root,
        patch_size=tuple(args.patch_size),
        mode='train'
    )
    
    # è°ƒè¯•æ¨¡å¼
    if args.debug:
        print("DEBUG MODE: Using only 10 samples")
        indices = list(range(min(10, len(full_dataset))))
        full_dataset = torch.utils.data.Subset(full_dataset, indices)
    
    # æ•°æ®åˆ’åˆ†
    train_size = int(0.9 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_set, val_set = random_split(full_dataset, [train_size, val_size])
    
    # éªŒè¯é›†ä½¿ç”¨ç‹¬ç«‹çš„ datasetï¼ˆä¸è†¨èƒ€ï¼‰æ¥ä¿æŒæŒ‡æ ‡çœŸå®æ€§
    val_dataset_clean = VesuviusDataset(
        csv_path=csv_path,
        image_root=image_root,
        label_root=label_root,
        patch_size=tuple(args.patch_size),
        mode='val'
    )
    # å–ç›¸åŒçš„éªŒè¯é›†ç´¢å¼•
    val_indices = val_set.indices
    val_set_clean = torch.utils.data.Subset(val_dataset_clean, val_indices)
    
    print(f"Train samples: {len(train_set)} (dilation={args.dilation_iters} on GPU), Val samples: {len(val_set_clean)} (no dilation)")
    
    # DataLoader (ä¼˜åŒ–: persistent_workers é¿å…é‡å»ºè¿›ç¨‹å¼€é”€, prefetch_factor é¢„åŠ è½½æ•°æ®)
    loader_kwargs = {
        'pin_memory': True,
        'num_workers': args.num_workers,
    }
    if args.num_workers > 0:
        loader_kwargs['persistent_workers'] = True
        loader_kwargs['prefetch_factor'] = 2
    
    train_loader = DataLoader(
        train_set, batch_size=args.batch_size, shuffle=True,
        drop_last=True, **loader_kwargs
    )
    val_loader = DataLoader(
        val_set_clean, batch_size=args.batch_size, shuffle=False,
        **loader_kwargs
    )
    
    # æ¨¡å‹ã€æŸå¤±ä¸ä¼˜åŒ–å™¨
    print("Initializing model...")
    if args.use_affinity:
        model = ResUNet3DWithAffinity(in_channels=1, out_channels=1).to(device)
        mamba_status = "Mamba SSM (çŠ¶æ€ç©ºé—´æ¨¡å‹)" if MAMBA_AVAILABLE else "å¤§æ ¸å·ç§¯å›é€€ (5x5x5 ç­‰æ•ˆæ„Ÿå—é‡)"
        print(f"[Model] ä½¿ç”¨ ResUNet3DWithAffinity | Bottleneck: {mamba_status} | Affinity Head: ON")
        criterion = CombinedLossWithAffinity(
            alpha=args.alpha, 
            beta=args.beta, 
            gamma=args.gamma,
            affinity_weight=args.affinity_weight,
            dynamic_beta=args.dynamic_beta,
            tversky_alpha=args.tversky_alpha,
            tversky_beta=args.tversky_beta
        ).to(device)
    else:
        model = ResUNet3D(in_channels=1, out_channels=1).to(device)
        print("[Model] ä½¿ç”¨æ ‡å‡† ResUNet3D")
        criterion = CombinedLoss(
            alpha=args.alpha, 
            beta=args.beta, 
            gamma=args.gamma,
            dynamic_beta=args.dynamic_beta,
            tversky_alpha=args.tversky_alpha,
            tversky_beta=args.tversky_beta
        ).to(device)
    
    # [P1/P2] CompoundLoss è¦†ç›– (æœ€é«˜ä¼˜å…ˆçº§)
    if args.use_compound:
        criterion = CompoundLoss(
            w_bce=args.w_bce,
            w_surface=args.w_surface,
            w_boundary=args.w_boundary,
            tau=args.tau,
            boundary_warmup=args.boundary_warmup
        ).to(device)
        print(f"[Loss] ä½¿ç”¨ CompoundLoss: BCE({args.w_bce}) + SurfaceDice({args.w_surface}, tau={args.tau}) + Boundary({args.w_boundary}, warmup={args.boundary_warmup})")
    
    optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    scaler = GradScaler()
    
    # LR è°ƒåº¦å™¨: å½“ Val Dice åœæ»æ—¶è‡ªåŠ¨é™ä½å­¦ä¹ ç‡
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', patience=5, factor=0.5, verbose=True
    )
    
    # æ–­ç‚¹ç»­è®­é€»è¾‘ (æ”¹è¿›ç‰ˆ: æ”¯æŒ strict=False éƒ¨åˆ†åŠ è½½ + --reset_optimizer)
    start_epoch = 1
    best_dice = 0.0
    if args.resume:
        if os.path.isfile(args.resume):
            print(f"[Resume] åŠ è½½ checkpoint: {args.resume}")
            checkpoint = torch.load(args.resume, map_location=device)
            
            # æ”¯æŒ strict=False éƒ¨åˆ†åŠ è½½ï¼ˆä»æ—§ ResUNet3D è¿ç§»åˆ° Affinity æ¨¡å‹ï¼‰
            load_result = model.load_state_dict(checkpoint['model_state_dict'], strict=False)
            if load_result.missing_keys:
                print(f"[Resume] æœªåŠ è½½çš„ key (æ–°å¢å±‚, éšæœºåˆå§‹åŒ–): {load_result.missing_keys}")
            if load_result.unexpected_keys:
                print(f"[Resume] æ„å¤–çš„ key (å·²å¿½ç•¥): {load_result.unexpected_keys}")
            
            if args.reset_optimizer:
                print(f"[Resume] --reset_optimizer: é‡ç½®ä¼˜åŒ–å™¨/è°ƒåº¦å™¨/Epoch (æ‰‹æœ¯å¼å¾®è°ƒæ¨¡å¼)")
                # ä¸åŠ è½½ optimizer, ä¸åŠ è½½ epoch, ä¸åŠ è½½ best_dice
                # Epoch é‡ç½®ä¸º 1, best_dice é‡ç½®ä¸º 0 (æ–°é˜¶æ®µé‡æ–°å¼€å§‹)
                start_epoch = 1
                best_dice = 0.0
            else:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                print(f"[Resume] åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€")
                start_epoch = checkpoint.get('epoch', 0) + 1
                best_dice = checkpoint.get('best_dice', 0.0)
            
            print(f"[Resume] ä» Epoch {start_epoch} å¼€å§‹è®­ç»ƒ, Best Dice: {best_dice:.4f}")
        else:
            print(f"[Resume] è­¦å‘Š: checkpoint æ–‡ä»¶ä¸å­˜åœ¨: {args.resume}")
    
    # è®­ç»ƒå¾ªç¯
    start_time = time.time()
    GREEN = '\033[92m'
    RESET = '\033[0m'
    current_accumulation_steps = args.accumulation_steps
    
    print(f"\n{'='*60}")
    print(f"Start training: Epoch {start_epoch} to {args.epochs}")
    if args.use_compound:
        print(f"Loss Config: CompoundLoss [BCE={args.w_bce}, SurfaceDice={args.w_surface}, Boundary={args.w_boundary}]")
        print(f"SurfaceDice tau={args.tau}, Boundary warmup={args.boundary_warmup} epochs")
    else:
        print(f"Loss Config: Alpha={args.alpha}, Beta={args.beta}, Gamma={args.gamma}")
        print(f"Tversky Config: alpha={args.tversky_alpha} (FP), beta={args.tversky_beta} (FN)")
    print(f"Affinity: {'ON (weight=' + str(args.affinity_weight) + ')' if args.use_affinity else 'OFF'}")
    print(f"Scheduler: ReduceLROnPlateau (patience=5, factor=0.5)")
    if not args.use_compound:
        print(f"Warmup: {args.cldice_warmup} epochs (Gamma=0)")
    print(f"{'='*60}")
    
    for epoch in range(start_epoch, args.epochs + 1):
        epoch_start = time.time()
        
        # [Output] Print LR (Decimal Format)
        current_lr = optimizer.param_groups[0]['lr']
        print(f"\n--- Epoch {epoch}/{args.epochs} | Learning Rate: {current_lr:.6f} ---")
        
        # Train & Val (Updated unpacking)
        (train_loss, t_bce, t_dice_loss, t_cldice) = train_epoch(
            model, train_loader, optimizer, criterion, scaler, device, 
            accumulation_steps=current_accumulation_steps,
            epoch=epoch,
            args=args
        )
        (val_loss, v_bce, v_dice_loss, v_cldice, val_dice_score, vis_samples) = validate(
            model, val_loader, criterion, device, use_affinity=args.use_affinity
        )
        
        epoch_duration = time.time() - epoch_start
        current_lr = optimizer.param_groups[0]['lr']
        
        # [Dynamic Strategy] è·å– GPU çŠ¶æ€å¹¶å»ºè®®ç­–ç•¥
        gpu_status = gpu_manager.get_status()
        current_accumulation_steps = gpu_manager.suggest_accumulation_steps(
            current_accumulation_steps, gpu_status['memory_util']
        )
        
        # è®°å½•æ—¥å¿— (æ‰©å±•å­—æ®µ)
        exp.log_epoch({
            "epoch": epoch,
            "train_loss": train_loss,
            "val_loss": val_loss,
            "val_dice": val_dice_score,
            "clDice": v_cldice, # è®°å½•éªŒè¯é›† clDice
            "lr": current_lr,
            "time": epoch_duration
        })
        
        # å®æ—¶ç»˜å›¾ (å¸¦ Patch å¯è§†åŒ–)
        patch_data = vis_samples[0] if vis_samples else None
        plot_training_curves(exp.get_log_path(), exp.get_exp_dir(), patch_data=patch_data)
        # å¤š Patch å¯¹æ¯”å›¾
        plot_multi_patch_comparison(vis_samples, exp.get_exp_dir(), epoch=epoch)
        
        # [Output] 3D Voxel Visualization (Raw/Label/Pred)
        if len(vis_samples) > 0:
             plot_3d_comparison(vis_samples[0], exp.get_exp_dir(), epoch=epoch, sample_idx=0)
             
             # [NEW] ç”Ÿæˆ 256x256x256 é«˜æ¸…æ ·æœ¬ (æ»‘åŠ¨çª—å£)
             highres_save_dir = Path(exp.get_exp_dir()) / "epoch_vis"
             highres_save_dir.mkdir(exist_ok=True)
             highres_path = highres_save_dir / f"epoch{epoch:03d}_highres_256.tif"
             
             # æ³¨æ„ï¼špatch_size åº”è¯¥ä½¿ç”¨è®­ç»ƒæ—¶çš„ patch_size (args.patch_size[0])
             generate_high_res_sample(
                 model=model,
                 dataset=val_dataset_clean, # ä½¿ç”¨éªŒè¯é›†æ•°æ®
                 save_path=highres_path,
                 device=device,
                 patch_size=args.patch_size[0], # Assuming cube
                 roi_size=256,
                 stride=32
             )

        
        # å‘½ä»¤è¡Œç›‘æ§è¾“å‡º (ä¼˜åŒ–ç‰ˆ)
        dice_str = f"{val_dice_score:.2f}" if val_dice_score > 0 else "0.00"
        if val_dice_score > best_dice:
            dice_str = f"{GREEN}{val_dice_score:.2f} (New Best!){RESET}"
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            save_path = exp.get_checkpoint_path("best_model.pth")
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_dice': val_dice_score,
                'args': vars(args)
            }, save_path)
            best_dice = val_dice_score

        # æ¯ä¸ª Epoch éƒ½ä¿å­˜ checkpoint (ç”¨äºåç»­é›†æˆé€‰æ‹©)
        epoch_save_path = exp.get_checkpoint_path(f"epoch_{epoch:03d}.pth")
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'best_dice': best_dice,
            'val_dice': val_dice_score,
            'val_loss': val_loss,
            'train_loss': train_loss,
            'args': vars(args)
        }, epoch_save_path)
            
        # è°ƒåº¦å™¨æ›´æ–°
        scheduler.step(val_dice_score)
        
        # [Output] Epoch æ€»ç»“ (å¯¹æ ‡ 20_src)
        # å°† Train Dice Loss è½¬æ¢ä¸ºè¿‘ä¼¼ Score ä»¥ä¾¿äºç›´è§‚å¯¹æ¯”
        train_dice_score_est = 1.0 - t_dice_loss
        
        print(
            f"\n  ğŸ“Š Epoch {epoch} æ€»ç»“:"
            f"\n     Train - Loss: {train_loss:.4f} | BCE: {t_bce:.4f} | Dice(Soft): {train_dice_score_est:.4f} | clDice: {t_cldice:.4f}"
            f"\n     Val   - Loss: {val_loss:.4f} | BCE: {v_bce:.4f} | Dice(Hard): {dice_str}"
            f"\n     Time: {format_time(epoch_duration)} | LR: {current_lr:.2e} | GPU Mem: {gpu_status['memory_util']:.1f}%"
        )
            
    total_time = time.time() - start_time
    print(f"\n{'='*60}")
    print(f"Training completed in {format_time(total_time)}")
    print(f"Best Validation Dice: {best_dice:.4f}")
    print(f"Experiment saved to: {exp.get_exp_dir()}")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    main()
