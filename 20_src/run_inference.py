"""
Vesuvius Challenge - GPU æ¨ç†è„šæœ¬ v2

æ ¸å¿ƒæ”¹è¿›ï¼š
- Sliding Window åˆ†å—æ¨ç†ï¼šé¿å… 320Â³ ä½“ç§¯ç›´æ¥é€å…¥ GPU å¯¼è‡´ OOM
- å®æ—¶è¿›åº¦ç›‘æ§ï¼šGPU æ˜¾å­˜ã€å¤„ç†é€Ÿåº¦ã€ETA
- è§„èŒƒåŒ–è¾“å‡ºç›®å½•å‘½åï¼šchimera_run_{æ—¥æœŸ}_{æ¨¡å¼}

ç”¨æ³•:
    python 20_src/run_inference.py --max_chunks 5
    python 20_src/run_inference.py --checkpoint path/to/model.pth
"""

import os
import sys
import time
import argparse
import psutil
from pathlib import Path
from datetime import datetime, timedelta

import numpy as np
import torch
import torch.nn.functional as F
import tifffile

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ sys.path ä¸­
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from importlib import import_module

model_mod = import_module("20_src.20_model.dual_unet")
DualHeadResUNet3D = model_mod.DualHeadResUNet3D


# ===== ç›‘æ§å·¥å…· =====

def get_gpu_stats():
    """è·å– GPU æ˜¾å­˜ä½¿ç”¨ä¿¡æ¯"""
    if not torch.cuda.is_available():
        return "CPU æ¨¡å¼"
    allocated = torch.cuda.memory_allocated() / 1024**3
    reserved = torch.cuda.memory_reserved() / 1024**3
    total = torch.cuda.get_device_properties(0).total_memory / 1024**3
    return f"GPU: {allocated:.1f}G/{total:.1f}G (reserved {reserved:.1f}G)"


def get_ram_stats():
    """è·å– RAM ä½¿ç”¨ä¿¡æ¯"""
    mem = psutil.virtual_memory()
    used_gb = mem.used / 1024**3
    total_gb = mem.total / 1024**3
    return f"RAM: {used_gb:.1f}G/{total_gb:.1f}G ({mem.percent}%)"


def format_eta(seconds):
    """æ ¼å¼åŒ–å‰©ä½™æ—¶é—´"""
    if seconds < 0:
        return "è®¡ç®—ä¸­..."
    td = timedelta(seconds=int(seconds))
    return str(td)


def print_progress(current, total, chunk_id, stage, extra=""):
    """æ‰“å°å®æ—¶è¿›åº¦"""
    pct = current / total * 100 if total > 0 else 0
    bar_len = 30
    filled = int(bar_len * current / total) if total > 0 else 0
    bar = "â–ˆ" * filled + "â–‘" * (bar_len - filled)

    timestamp = datetime.now().strftime("%H:%M:%S")
    line = f"[{timestamp}] [{bar}] {current}/{total} ({pct:.0f}%) | ID:{chunk_id} | {stage}"
    if extra:
        line += f" | {extra}"
    print(line, flush=True)


# ===== åˆ†å—æ¨ç†å¼•æ“ =====

def sliding_window_inference(
    model,
    volume: torch.Tensor,
    patch_size: tuple = (64, 128, 128),
    overlap: int = 8,
    device: torch.device = None,
):
    """
    Sliding Window åˆ†å—æ¨ç†

    å°†å¤§ä½“ç§¯åˆ‡æˆå° patchï¼Œé€ä¸ªé€å…¥ GPU æ¨ç†ï¼Œå†æ‹¼å›æ¥ã€‚
    é‡å åŒºåŸŸä½¿ç”¨å¹³å‡èåˆã€‚

    Args:
        model: DualHeadResUNet3D
        volume: (1, 1, D, H, W) å®Œæ•´è¾“å…¥ä½“ç§¯
        patch_size: æ¯ä¸ª patch çš„å¤§å° (pD, pH, pW)
        overlap: patch ä¹‹é—´çš„é‡å ä½“ç´ æ•°
        device: æ¨ç†è®¾å¤‡

    Returns:
        seg_prob: (D, H, W) åˆ†å‰²æ¦‚ç‡å›¾
        normal_map: (3, D, H, W) æ³•çº¿å›¾
    """
    _, _, D, H, W = volume.shape
    pD, pH, pW = patch_size
    stride_d = pD - overlap
    stride_h = pH - overlap
    stride_w = pW - overlap

    # è¾“å‡ºç´¯ç§¯å™¨
    seg_sum = torch.zeros(1, 1, D, H, W, dtype=torch.float32)
    normal_sum = torch.zeros(1, 3, D, H, W, dtype=torch.float32)
    count = torch.zeros(1, 1, D, H, W, dtype=torch.float32)

    # è®¡ç®—æ‰€æœ‰ patch çš„èµ·å§‹ä½ç½®
    d_starts = list(range(0, max(D - pD + 1, 1), stride_d))
    h_starts = list(range(0, max(H - pH + 1, 1), stride_h))
    w_starts = list(range(0, max(W - pW + 1, 1), stride_w))

    # ç¡®ä¿è¦†ç›–è¾¹ç•Œ
    if d_starts[-1] + pD < D:
        d_starts.append(D - pD)
    if h_starts[-1] + pH < H:
        h_starts.append(H - pH)
    if w_starts[-1] + pW < W:
        w_starts.append(W - pW)

    total_patches = len(d_starts) * len(h_starts) * len(w_starts)
    patch_idx = 0

    for d0 in d_starts:
        for h0 in h_starts:
            for w0 in w_starts:
                patch_idx += 1

                # æå– patch
                d1 = min(d0 + pD, D)
                h1 = min(h0 + pH, H)
                w1 = min(w0 + pW, W)

                patch = volume[:, :, d0:d1, h0:h1, w0:w1]

                # Pad å¦‚æœå°ºå¯¸ä¸å¤Ÿ
                actual_d, actual_h, actual_w = d1 - d0, h1 - h0, w1 - w0
                if actual_d < pD or actual_h < pH or actual_w < pW:
                    pad_d = pD - actual_d
                    pad_h = pH - actual_h
                    pad_w = pW - actual_w
                    patch = F.pad(patch, (0, pad_w, 0, pad_h, 0, pad_d))

                # GPU æ¨ç†
                patch_gpu = patch.to(device)
                seg_logits, normals = model(patch_gpu)
                seg_prob = torch.sigmoid(seg_logits)

                # è£å‰ªå›å®é™…å°ºå¯¸
                seg_prob = seg_prob[:, :, :actual_d, :actual_h, :actual_w].cpu()
                normals = normals[:, :, :actual_d, :actual_h, :actual_w].cpu()

                # ç´¯åŠ 
                seg_sum[:, :, d0:d1, h0:h1, w0:w1] += seg_prob
                normal_sum[:, :, d0:d1, h0:h1, w0:w1] += normals
                count[:, :, d0:d1, h0:h1, w0:w1] += 1.0

                # æ¸…ç† GPU æ˜¾å­˜
                del patch_gpu, seg_logits, seg_prob, normals
                if device.type == "cuda":
                    torch.cuda.empty_cache()

                # æ¯ 5 ä¸ª patch æ‰“å°ä¸€æ¬¡è¿›åº¦
                if patch_idx % 5 == 0 or patch_idx == total_patches:
                    print(f"    patch {patch_idx}/{total_patches} | {get_gpu_stats()}", flush=True)

    # å¹³å‡èåˆ
    count = count.clamp(min=1.0)
    seg_avg = (seg_sum / count).squeeze().numpy()       # (D, H, W)
    normal_avg = (normal_sum / count).squeeze(0).numpy() # (3, D, H, W)

    return seg_avg, normal_avg


# ===== ä¸»æ¨ç†å‡½æ•° =====

def run_inference(args):
    """ä¸»æ¨ç†æµç¨‹"""

    # è®¾å¤‡
    if args.device == "auto":
        dev = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        dev = torch.device(args.device)

    # è§„èŒƒåŒ–è¾“å‡ºç›®å½•å‘½å: chimera_run_{æ—¥æœŸ}_{æ¨¡å¼}
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    mode_tag = "gpu" if dev.type == "cuda" else "cpu"
    run_name = f"chimera_run_{timestamp}_{mode_tag}_nf{args.n_filters}"
    run_output_dir = Path(args.output_dir) / run_name
    run_output_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n{'='*70}")
    print(f"  ğŸ”¬ Hybrid Chimera MVP - æ¨ç†")
    print(f"  è®¾å¤‡: {dev} | {get_gpu_stats()}")
    print(f"  {get_ram_stats()}")
    print(f"  Patch å¤§å°: {args.patch_size}")
    print(f"  è¾“å…¥: {args.input_dir}")
    print(f"  è¾“å‡º: {run_output_dir}")
    print(f"{'='*70}\n")

    # åŠ è½½æ¨¡å‹
    model = DualHeadResUNet3D(in_channels=1, n_filters=args.n_filters)
    if args.checkpoint and os.path.exists(args.checkpoint):
        state_dict = torch.load(args.checkpoint, map_location=dev, weights_only=True)
        if any(k.startswith("model.") for k in state_dict.keys()):
            state_dict = {k.replace("model.", ""): v for k, v in state_dict.items()}
        model.load_state_dict(state_dict, strict=False)
        print(f"[Model] å·²åŠ è½½æƒé‡: {args.checkpoint}")
    else:
        print("[Model] ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ï¼ˆMVP æ¼”ç¤ºæ¨¡å¼ï¼‰")

    model.to(dev)
    model.eval()
    params = sum(p.numel() for p in model.parameters())
    print(f"[Model] å‚æ•°é‡: {params:,} | {get_gpu_stats()}\n")

    # æ‰«æè¾“å…¥
    input_path = Path(args.input_dir)
    tif_files = sorted([
        f for f in input_path.iterdir()
        if f.suffix.lower() in ('.tif', '.tiff')
    ])
    if args.max_chunks:
        tif_files = tif_files[:args.max_chunks]

    total = len(tif_files)
    print(f"[Data] {total} ä¸ª .tif æ–‡ä»¶å¾…å¤„ç†\n")

    # æ¨ç†å¾ªç¯
    all_times = []
    normal_stats = []

    with torch.no_grad():
        for idx, tif_file in enumerate(tif_files):
            chunk_id = tif_file.stem
            print_progress(idx + 1, total, chunk_id, "å¼€å§‹åŠ è½½")

            t_start = time.time()

            # 1. åŠ è½½
            t0 = time.time()
            volume_raw = tifffile.imread(str(tif_file))
            vol_shape = volume_raw.shape
            volume = volume_raw.astype(np.float32)
            if volume.max() > 1.0:
                volume = volume / 255.0 if volume.max() <= 255.0 else volume / 65535.0
            volume = np.clip(volume, 0.0, 1.0)
            x = torch.from_numpy(volume).unsqueeze(0).unsqueeze(0)  # (1,1,D,H,W)
            t_load = time.time() - t0

            print(f"    å½¢çŠ¶: {vol_shape} | åŠ è½½: {t_load:.2f}s | {get_ram_stats()}", flush=True)

            # 2. Sliding Window æ¨ç†
            t0 = time.time()
            patch_size = tuple(args.patch_size)
            seg_prob, normal_map = sliding_window_inference(
                model, x, patch_size=patch_size,
                overlap=args.overlap, device=dev,
            )
            t_unet = time.time() - t0

            # 3. æ³•å‘é‡è´¨é‡è¯Šæ–­
            # è®¡ç®—æ³•çº¿æ¨¡é•¿ (åº”æ¥è¿‘ 1.0) å’Œæ–¹å‘ä¸€è‡´æ€§
            norm_magnitude = np.linalg.norm(normal_map, axis=0)  # (D,H,W)
            mask_region = seg_prob > 0.3  # åªåœ¨æœ‰æ„ä¹‰çš„åŒºåŸŸç»Ÿè®¡
            if mask_region.sum() > 0:
                avg_norm = norm_magnitude[mask_region].mean()
                std_norm = norm_magnitude[mask_region].std()
                # æ–¹å‘ä¸€è‡´æ€§: é‚»å±…æ³•çº¿ç‚¹ç§¯çš„å‡å€¼
                normal_stats.append({
                    "id": chunk_id,
                    "avg_norm_magnitude": float(avg_norm),
                    "std_norm_magnitude": float(std_norm),
                })
                norm_diag = f"æ³•çº¿æ¨¡é•¿: {avg_norm:.3f}Â±{std_norm:.3f}"
            else:
                norm_diag = "æ³•çº¿: æ— æœ‰æ•ˆåŒºåŸŸ"

            # 4. é˜ˆå€¼åŒ–ç”Ÿæˆ mask
            t0 = time.time()
            final_mask = (seg_prob > 0.5).astype(np.uint8)
            t_post = time.time() - t0

            t_total = time.time() - t_start
            all_times.append(t_total)

            # 5. ä¿å­˜
            output_filename = f"{chunk_id}_mask.tif"
            output_path = run_output_dir / output_filename
            tifffile.imwrite(str(output_path), final_mask)

            # è®¡ç®— ETA
            avg_time = sum(all_times) / len(all_times)
            eta = avg_time * (total - idx - 1)
            mask_pct = final_mask.sum() / final_mask.size * 100

            print(f"    U-Net: {t_unet:.1f}s | åå¤„ç†: {t_post:.3f}s | "
                  f"æ€»è®¡: {t_total:.1f}s", flush=True)
            print(f"    Mask: {mask_pct:.1f}% | {norm_diag}", flush=True)
            print(f"    {get_gpu_stats()} | {get_ram_stats()}", flush=True)
            print(f"    ETA: {format_eta(eta)} | å¹³å‡: {avg_time:.1f}s/chunk", flush=True)
            print(f"    â†’ {output_path.name}", flush=True)
            print("", flush=True)

            # æ¸…ç†
            del x, volume, volume_raw
            if dev.type == "cuda":
                torch.cuda.empty_cache()

    # ===== æœ€ç»ˆæŠ¥å‘Š =====
    total_time = sum(all_times)
    avg_time = total_time / max(len(all_times), 1)

    print(f"\n{'='*70}")
    print(f"  æ¨ç†å®Œæˆ!")
    print(f"  å¤„ç†: {len(all_times)} chunks")
    print(f"  æ€»è€—æ—¶: {total_time:.1f}s ({total_time/60:.1f} min)")
    print(f"  å¹³å‡: {avg_time:.1f}s/chunk")
    print(f"  è¾“å‡º: {run_output_dir}")

    # æ³•å‘é‡è´¨é‡è¯Šæ–­æ±‡æ€»
    if normal_stats:
        avg_mag = np.mean([s['avg_norm_magnitude'] for s in normal_stats])
        avg_std = np.mean([s['std_norm_magnitude'] for s in normal_stats])
        print(f"\n  ğŸ“ æ³•å‘é‡è¯Šæ–­:")
        print(f"    å¹³å‡æ¨¡é•¿: {avg_mag:.3f} (ç†æƒ³=1.0)")
        print(f"    æ¨¡é•¿æ ‡å‡†å·®: {avg_std:.3f} (è¶Šä½è¶Šå¥½)")
        if avg_mag < 0.5:
            print(f"    âš ï¸ æ³•çº¿æ–¹å‘æ‚ä¹±ï¼Œå»ºè®®åŠ å¼º L_Cosine æƒé‡ (å½“å‰ 0.1 â†’ 0.5)")

    # å†…å­˜è¯Šæ–­
    print(f"\n  ğŸ’¾ èµ„æºè¯Šæ–­:")
    print(f"    {get_ram_stats()}")
    print(f"    {get_gpu_stats()}")
    ram_gb = psutil.virtual_memory().used / 1024**3
    if ram_gb > 16:
        print(f"    âš ï¸ RAM ä½¿ç”¨è¶…è¿‡ 16GBï¼Œå»ºè®®åŠ å…¥ supervoxel é™é‡‡æ ·")

    # æ—¶é—´è¯Šæ–­
    print(f"\n  â±ï¸ æ€§èƒ½è¯Šæ–­:")
    print(f"    å• chunk å¹³å‡: {avg_time:.1f}s")
    if avg_time > 900:  # 15 åˆ†é’Ÿ
        print(f"    âš ï¸ å• chunk > 15 minï¼Œå¿…é¡»åˆ‡æ¢åˆ° CuPy GPU æ±‚è§£å™¨")
    else:
        print(f"    âœ“ å• chunk æ—¶é—´å¯æ¥å—")

    estimated_full = avg_time * 786 / 60
    print(f"    é¢„ä¼°å…¨é‡æ¨ç†: {estimated_full:.0f} min ({estimated_full/60:.1f} h)")
    print(f"{'='*70}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Hybrid Chimera - GPU æ¨ç† v2")
    parser.add_argument("--input_dir", type=str,
                        default="data/vesuvius-challenge-surface-detection/train_images")
    parser.add_argument("--output_dir", type=str, default="20_src/output")
    parser.add_argument("--checkpoint", type=str, default=None)
    parser.add_argument("--device", type=str, default="auto")
    parser.add_argument("--n_filters", type=int, default=16)
    parser.add_argument("--max_chunks", type=int, default=None)
    parser.add_argument("--patch_size", type=int, nargs=3, default=[64, 128, 128],
                        help="Sliding window patch å¤§å° (D H W)")
    parser.add_argument("--overlap", type=int, default=8,
                        help="Patch é‡å ä½“ç´ æ•°")

    args = parser.parse_args()
    run_inference(args)
