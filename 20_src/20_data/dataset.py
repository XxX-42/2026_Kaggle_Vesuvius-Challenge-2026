"""
Vesuvius Challenge - 3D TIF Chunk æ•°æ®åŠ è½½å™¨

åŒ…å«ä¸¤ä¸ª Datasetï¼š
- TifChunkDataset:       æ¨ç†ç”¨ï¼Œæ•´ä½“åŠ è½½
- VesuviusTrainDataset:  è®­ç»ƒç”¨ï¼ŒNPY mmap é›¶æ‹·è´ / TIF LRU ç¼“å­˜

æ ¸å¿ƒä¼˜åŒ–ï¼ˆNPY æ¨¡å¼ï¼‰ï¼š
  np.load(mmap_mode='r') å°†æ–‡ä»¶æ˜ å°„åˆ°è™šæ‹Ÿå†…å­˜ï¼Œ
  åªæœ‰åœ¨ slice æ—¶æ‰è§¦å‘ç¼ºé¡µä¸­æ–­è¯»å–å¯¹åº”çš„å­—èŠ‚ã€‚
  96Â³ uint8 patch â‰ˆ 0.88MBï¼ŒIO æ—¶é—´ < 1msã€‚
"""

import os
from pathlib import Path
from typing import Optional, Callable, List, Union
from collections import OrderedDict

import numpy as np
import torch
from torch.utils.data import Dataset
import tifffile


class TifChunkDataset(Dataset):
    """
    3D TIF Chunk æ•°æ®é›†ï¼ˆæ¨ç†ç”¨ï¼‰

    ä»æŒ‡å®šç›®å½•ä¸­æ‰«ææ‰€æœ‰ .tif/.tiff æ–‡ä»¶ï¼Œé€ä¸ªæ•´ä½“åŠ è½½ä¸º 3D tensorã€‚
    """

    def __init__(
        self,
        data_source: Union[str, Path, List[str]],
        transform: Optional[Callable] = None,
        normalize: bool = True,
    ):
        super().__init__()
        self.transform = transform
        self.normalize = normalize

        if isinstance(data_source, (str, Path)):
            data_source = Path(data_source)
            if data_source.is_dir():
                self.file_paths = sorted([
                    str(p) for p in data_source.iterdir()
                    if p.suffix.lower() in ('.tif', '.tiff')
                ])
            elif data_source.is_file():
                self.file_paths = [str(data_source)]
            else:
                raise FileNotFoundError(f"æ•°æ®æºä¸å­˜åœ¨: {data_source}")
        elif isinstance(data_source, list):
            self.file_paths = [str(p) for p in data_source]
        else:
            raise TypeError(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {type(data_source)}")

        if len(self.file_paths) == 0:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½• .tif æ–‡ä»¶")

        print(f"[TifChunkDataset] åŠ è½½äº† {len(self.file_paths)} ä¸ª chunk æ–‡ä»¶")

    def __len__(self) -> int:
        return len(self.file_paths)

    def __getitem__(self, idx: int) -> torch.Tensor:
        file_path = self.file_paths[idx]
        volume = tifffile.imread(file_path)

        if volume.ndim == 2:
            volume = volume[np.newaxis, ...]
        elif volume.ndim != 3:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç»´åº¦ {volume.ndim}Dï¼Œæ–‡ä»¶: {file_path}")

        volume = volume.astype(np.float32)
        if self.normalize:
            volume = self._normalize(volume)
        if self.transform is not None:
            volume = self.transform(volume)

        return torch.from_numpy(volume).unsqueeze(0)

    def _normalize(self, data: np.ndarray) -> np.ndarray:
        max_val = data.max()
        if max_val <= 1.0 + 1e-6:
            return np.clip(data, 0.0, 1.0)
        elif max_val <= 255.0 + 1e-6:
            return data / 255.0
        else:
            return data / 65535.0

    def get_file_path(self, idx: int) -> str:
        return self.file_paths[idx]


# ===================================================================
#  é«˜æ€§èƒ½è®­ç»ƒ Dataset
# ===================================================================

class VesuviusTrainDataset(Dataset):
    """
    Vesuvius è®­ç»ƒæ•°æ®é›†ï¼ˆé«˜æ€§èƒ½ç‰ˆï¼‰

    è‡ªåŠ¨æ£€æµ‹é¢„å¤„ç†çš„ NPY æ–‡ä»¶ï¼Œä¼˜å…ˆä½¿ç”¨ mmap é›¶æ‹·è´æ¨¡å¼ã€‚
    è‹¥ NPY ä¸å­˜åœ¨åˆ™å›é€€åˆ° TIF + LRU ç¼“å­˜ã€‚

    æ€§èƒ½å¯¹æ¯”ï¼š
      NPY mmap: ~0.1ms/sample (é›¶æ‹·è´åˆ‡ç‰‡)
      TIF cache hit: ~0.1ms/sample (å†…å­˜ç¼“å­˜)
      TIF cache miss: ~100ms/sample (è§£å‹ LZW)

    Args:
        image_dir:  train_images ç›®å½•
        label_dir:  train_labels ç›®å½•
        crop_size:  3D éšæœºè£å‰ªå°ºå¯¸
        transform:  è£å‰ªåçš„å¢å¼ºå˜æ¢ (æ¥å— (image, label) è¿”å› (image, label))
        samples_per_volume: æ¯ä¸ªä½“ç§¯æ¯ epoch é‡‡å‡ ä¸ª patch
        cache_size: TIF æ¨¡å¼ä¸‹çš„ LRU ç¼“å­˜ä½“ç§¯æ•°é‡
        max_files:  æœ€å¤šä½¿ç”¨å¤šå°‘ä¸ªæ–‡ä»¶ï¼ˆNone=å…¨éƒ¨ï¼‰
    """

    def __init__(
        self,
        image_dir: Union[str, Path],
        label_dir: Union[str, Path],
        crop_size: int = 96,
        transform: Optional[Callable] = None,
        samples_per_volume: int = 16,
        cache_size: int = 32,
        max_files: Optional[int] = None,
        pos_ratio: float = 0.5,
    ):
        super().__init__()
        self.transform = transform
        self.samples_per_volume = samples_per_volume
        self.cache_size = cache_size
        self.pos_ratio = pos_ratio  # æ­£æ ·æœ¬å¼ºåˆ¶é‡‡æ ·æ¯”ä¾‹ï¼ˆæ‹’ç»é‡‡æ ·ï¼‰

        # è£å‰ªå°ºå¯¸
        if isinstance(crop_size, int):
            self.crop_size = (crop_size, crop_size, crop_size)
        else:
            self.crop_size = tuple(crop_size)

        image_dir = Path(image_dir)
        label_dir = Path(label_dir)

        # ====== ä¼˜å…ˆæ£€æµ‹ NPY ç›®å½• ======
        npy_image_dir = image_dir.parent / (image_dir.name + "_npy")
        npy_label_dir = label_dir.parent / (label_dir.name + "_npy")

        self.use_npy = False
        if npy_image_dir.exists() and npy_label_dir.exists():
            image_files = {p.stem: p for p in sorted(npy_image_dir.glob("*.npy"))}
            label_files = {p.stem: p for p in sorted(npy_label_dir.glob("*.npy"))}
            if len(image_files) > 0 and len(label_files) > 0:
                self.use_npy = True
                print(f"[Dataset] ğŸš€ å‘ç°é¢„å¤„ç† NPY æ•°æ® "
                      f"({len(image_files)} img + {len(label_files)} lbl)ï¼Œ"
                      f"å¯ç”¨æé€Ÿ mmap æ¨¡å¼ï¼")
            else:
                print("[Dataset] NPY ç›®å½•ä¸ºç©ºï¼Œå›é€€åˆ° TIF æ¨¡å¼")

        if not self.use_npy:
            # å›é€€åˆ° TIF
            image_files = {
                p.stem: p for p in sorted(image_dir.iterdir())
                if p.suffix.lower() in ('.tif', '.tiff')
            }
            label_files = {
                p.stem: p for p in sorted(label_dir.iterdir())
                if p.suffix.lower() in ('.tif', '.tiff')
            }
            print(f"[Dataset] ä½¿ç”¨ TIF æ¨¡å¼ + LRU ç¼“å­˜ (cache_size={cache_size})")

        # é…å¯¹
        common_ids = sorted(set(image_files.keys()) & set(label_files.keys()))
        if max_files is not None:
            common_ids = common_ids[:max_files]

        self.pairs = [
            (str(image_files[cid]), str(label_files[cid]))
            for cid in common_ids
        ]

        if len(self.pairs) == 0:
            raise ValueError(
                f"æœªæ‰¾åˆ° image-label é…å¯¹ï¼\n"
                f"  image_dir: {image_dir}\n"
                f"  label_dir: {label_dir}"
            )

        # é¢„æ‰«æä½“ç§¯ shape
        self._shapes = []
        if self.use_npy:
            # NPY: è¯» header è·å– shapeï¼ˆæå¿«ï¼‰
            for img_path, _ in self.pairs:
                arr = np.load(img_path, mmap_mode='r')
                self._shapes.append(arr.shape)
        else:
            # TIF: è¯»æ–‡ä»¶å¤´
            for img_path, _ in self.pairs:
                with tifffile.TiffFile(img_path) as tif:
                    self._shapes.append(tif.series[0].shape)

        # TIF æ¨¡å¼ä¸‹çš„ LRU ç¼“å­˜
        self._cache = OrderedDict()

        print(f"[VesuviusTrainDataset] {len(self.pairs)} ä¸ªé…å¯¹, "
              f"crop={self.crop_size}, {samples_per_volume} samples/vol, "
              f"æ€»è®¡ {len(self)} ä¸ªè®­ç»ƒæ ·æœ¬/epoch")

    def __len__(self) -> int:
        return len(self.pairs) * self.samples_per_volume

    def _load_volume(self, vol_idx: int):
        """
        åŠ è½½ä½“ç§¯æ•°æ®

        NPY æ¨¡å¼: np.load(mmap_mode='r') â†’ è¿”å› mmap å¯¹è±¡ï¼Œé›¶ RAM å¼€é”€
        TIF æ¨¡å¼: imread + LRU ç¼“å­˜
        """
        if self.use_npy:
            img_path, lbl_path = self.pairs[vol_idx]
            image = np.load(img_path, mmap_mode='r')
            label = np.load(lbl_path, mmap_mode='r')
            return image, label

        # TIF LRU ç¼“å­˜
        if vol_idx in self._cache:
            self._cache.move_to_end(vol_idx)
            return self._cache[vol_idx]

        img_path, lbl_path = self.pairs[vol_idx]
        image = tifffile.imread(img_path)
        label = tifffile.imread(lbl_path)

        self._cache[vol_idx] = (image, label)
        if len(self._cache) > self.cache_size:
            self._cache.popitem(last=False)

        return image, label

    def _random_crop_coords(self, vol_shape):
        """è®¡ç®—éšæœºè£å‰ªåæ ‡ï¼Œå¤„ç†å°ä½“ç§¯ padding"""
        coords = []
        pads = []
        for dim_size, crop_dim in zip(vol_shape, self.crop_size):
            if dim_size >= crop_dim:
                start = np.random.randint(0, dim_size - crop_dim + 1)
                coords.append((start, start + crop_dim))
                pads.append((0, 0))
            else:
                coords.append((0, dim_size))
                pads.append((0, crop_dim - dim_size))
        return coords, pads

    def _surface_biased_crop(self, label_vol, vol_shape):
        """
        Surface-Biased Rejection Samplingï¼ˆè¡¨é¢åå‘æ‹’ç»é‡‡æ ·ï¼‰

        ä»¥ pos_ratio çš„æ¦‚ç‡å¼ºåˆ¶é‡‡æ ·åˆ°å«æ­£æ ·æœ¬ï¼ˆå¢¨æ°´/çº¸å¼ ï¼‰çš„åŒºåŸŸï¼Œ
        å‰©ä½™æ¦‚ç‡è¿›è¡Œçº¯éšæœºé‡‡æ ·ï¼ˆè´Ÿæ ·æœ¬æŒ–æ˜ï¼‰ã€‚

        IO å®‰å…¨ä¿è¯ï¼š
          - label_vol æ˜¯ np.memmap å¯¹è±¡ï¼ˆmmap_mode='r'ï¼‰
          - label_vol[d0:d1, h0:h1, w0:w1] åˆ‡ç‰‡ä»…è§¦å‘ç¼ºé¡µä¸­æ–­ï¼Œ
            æ“ä½œç³»ç»Ÿåªè¯»å–å¯¹åº”é¡µé¢ï¼ˆ~0.25MBï¼‰ï¼Œä¸åŠ è½½æ•´ä¸ªä½“ç§¯
          - np.any() çŸ­è·¯æ±‚å€¼ï¼Œå‘½ä¸­é¦–ä¸ªéé›¶å…ƒç´ å³è¿”å›

        Args:
            label_vol: label ä½“ç§¯ï¼ˆmmap å¯¹è±¡æˆ– ndarrayï¼‰
            vol_shape: ä½“ç§¯çš„å½¢çŠ¶ (D, H, W)

        Returns:
            coords, pads: ä¸ _random_crop_coords æ ¼å¼ä¸€è‡´
        """
        force_positive = (np.random.rand() < self.pos_ratio)

        if force_positive:
            # æ‹’ç»é‡‡æ ·ï¼šæœ€å¤šé‡è¯• 10 æ¬¡å¯»æ‰¾å«æ­£æ ·æœ¬çš„ patch
            for _attempt in range(10):
                coords, pads = self._random_crop_coords(vol_shape)
                (d0, d1), (h0, h1), (w0, w1) = coords
                # å…³é”®: ä»…å¯¹ label mmap åšåˆ‡ç‰‡ peekï¼Œä¸åŠ è½½æ•´ä¸ªä½“ç§¯
                label_patch = label_vol[d0:d1, h0:h1, w0:w1]
                if np.any(label_patch == 1):
                    return coords, pads
            # 10 æ¬¡å…¨å¤±è´¥ï¼ˆæç½•è§ï¼‰ï¼Œæ¥å—æœ€åä¸€æ¬¡çš„éšæœºåæ ‡
            return coords, pads
        else:
            # è´Ÿæ ·æœ¬æŒ–æ˜ï¼šçº¯éšæœºè£å‰ª
            return self._random_crop_coords(vol_shape)

    def __getitem__(self, idx: int):
        """
        åŠ è½½ä½“ç§¯ â†’ éšæœºè£å‰ª â†’ å½’ä¸€åŒ– â†’ å¢å¼º â†’ Tensor

        Returns:
            image: (1, cD, cH, cW) float32 [0, 1]
            label: (1, cD, cH, cW) float32 {0, 1}
        """
        vol_idx = idx // self.samples_per_volume
        image_vol, label_vol = self._load_volume(vol_idx)

        # éšæœºè£å‰ªï¼ˆimage å’Œ label ç”¨ç›¸åŒåæ ‡ï¼‰
        # ä½¿ç”¨ Surface-Biased Rejection Sampling æ›¿ä»£çº¯éšæœºè£å‰ª
        coords, pads = self._surface_biased_crop(label_vol, image_vol.shape)
        (d0, d1), (h0, h1), (w0, w1) = coords

        # åˆ‡ç‰‡ + è½¬ float32ï¼ˆNPY mmap æ­¤æ—¶æ‰è§¦å‘çœŸæ­£çš„ç£ç›˜è¯»å–ï¼‰
        image = np.array(image_vol[d0:d1, h0:h1, w0:w1], dtype=np.float32)
        label = np.array(label_vol[d0:d1, h0:h1, w0:w1], dtype=np.float32)

        # Paddingï¼ˆä½“ç§¯å°äº crop_size æ—¶ï¼‰
        need_pad = any(p != (0, 0) for p in pads)
        if need_pad:
            image = np.pad(image, pads, mode='constant', constant_values=0)
            label = np.pad(label, pads, mode='constant', constant_values=0)

        # å½’ä¸€åŒ– image â†’ [0, 1]
        max_val = image.max()
        if max_val > 1.0:
            if max_val <= 255.0:
                image = image / 255.0
            else:
                image = image / 65535.0
        image = np.clip(image, 0.0, 1.0)

        # Label äºŒå€¼åŒ–ï¼ˆæ–¹æ¡ˆ Aï¼šåªè®¤ val=1 ä¸ºçº¸è‰è¡¨é¢ï¼Œval=2 ä¸ºå¿½ç•¥åŒºåŸŸå½“ä½œèƒŒæ™¯ï¼‰
        # ç«èµ›æ ‡ç­¾ï¼š0=èƒŒæ™¯, 1=çº¸è‰è¡¨é¢(ç›®æ ‡), 2=å¿½ç•¥/å¡«å……
        label = (label == 1).astype(np.float32)

        # å¢å¼ºå˜æ¢ï¼ˆä»… FlipRotateï¼ŒCrop å·²å®Œæˆï¼‰
        if self.transform is not None:
            image, label = self.transform(image, label)

        # è½¬ Tensor
        image_t = torch.from_numpy(np.ascontiguousarray(image)).unsqueeze(0)
        label_t = torch.from_numpy(np.ascontiguousarray(label)).unsqueeze(0)

        return image_t, label_t


if __name__ == "__main__":
    import time

    print("=== Dataset è‡ªæµ‹ ===")
    test_dir = Path("__test_tif_chunks__")
    test_dir.mkdir(exist_ok=True)

    try:
        # TifChunkDataset æµ‹è¯•
        for i in range(3):
            tifffile.imwrite(
                str(test_dir / f"chunk_{i:03d}.tif"),
                np.random.randint(0, 65535, (16, 32, 32), dtype=np.uint16)
            )
        ds = TifChunkDataset(test_dir)
        sample = ds[0]
        assert sample.shape == (1, 16, 32, 32)
        print("âœ“ TifChunkDataset é€šè¿‡ï¼")

        # VesuviusTrainDataset æµ‹è¯• (TIF æ¨¡å¼)
        img_dir = test_dir / "images"
        lbl_dir = test_dir / "labels"
        img_dir.mkdir(exist_ok=True)
        lbl_dir.mkdir(exist_ok=True)
        for i in range(5):
            tifffile.imwrite(str(img_dir / f"vol_{i:03d}.tif"),
                             np.random.randint(0, 255, (64, 64, 64), dtype=np.uint8))
            tifffile.imwrite(str(lbl_dir / f"vol_{i:03d}.tif"),
                             np.random.choice([0, 1, 2], (64, 64, 64)).astype(np.uint8))

        train_ds = VesuviusTrainDataset(
            img_dir, lbl_dir, crop_size=32, samples_per_volume=4, cache_size=5
        )

        t0 = time.time()
        for i in range(20):
            img, lbl = train_ds[i % len(train_ds)]
        t1 = time.time()
        print(f"TIF æ¨¡å¼: {(t1-t0)/20*1000:.1f}ms/sample, "
              f"image={img.shape}, label={lbl.shape}")

        # VesuviusTrainDataset æµ‹è¯• (NPY æ¨¡å¼)
        npy_img = test_dir / "images_npy"
        npy_lbl = test_dir / "labels_npy"
        npy_img.mkdir(exist_ok=True)
        npy_lbl.mkdir(exist_ok=True)
        for i in range(5):
            np.save(str(npy_img / f"vol_{i:03d}.npy"),
                    np.random.randint(0, 255, (64, 64, 64), dtype=np.uint8))
            np.save(str(npy_lbl / f"vol_{i:03d}.npy"),
                    np.random.choice([0, 1, 2], (64, 64, 64)).astype(np.uint8))

        npy_ds = VesuviusTrainDataset(
            img_dir, lbl_dir, crop_size=32, samples_per_volume=4
        )

        t0 = time.time()
        for i in range(20):
            img, lbl = npy_ds[i % len(npy_ds)]
        t1 = time.time()
        print(f"NPY æ¨¡å¼: {(t1-t0)/20*1000:.1f}ms/sample, "
              f"image={img.shape}, label={lbl.shape}")

        print("\nâœ“ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")

    finally:
        import shutil
        if test_dir.exists():
            shutil.rmtree(test_dir)
