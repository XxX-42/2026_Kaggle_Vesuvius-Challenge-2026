"""
Vesuvius Challenge - Hybrid Chimera è®­ç»ƒå¼•æ“ (Phase 5)

Patch-based 3D è®­ç»ƒå¾ªç¯ï¼Œæ”¯æŒï¼š
- AMP æ··åˆç²¾åº¦ (FP16)
- Random Crop 3D + æ•°æ®å¢å¼º
- ChimeraLoss (Dice + Normal Cosine)
- tqdm è¿›åº¦æ¡
- æ¯ epoch å¯è§†åŒ–è¾“å‡º (PNG å¯¹æ¯”å›¾ + TIF mask)
- éªŒè¯ Dice ç›‘æ§ + Best Model ä¿å­˜

ç”¨æ³•:
    # å¿«é€Ÿæµ‹è¯• (5 ä¸ª chunk, 2 ä¸ª epoch)
    python 20_src/train.py --max_chunks 5 --epochs 2

    # å®Œæ•´è®­ç»ƒ
    python 20_src/train.py --epochs 50 --batch_size 4 --lr 1e-3

    # ä» checkpoint æ¢å¤
    python 20_src/train.py --resume 20_src/output/best_model.pth --epochs 50
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path
from datetime import datetime, timedelta

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from tqdm import tqdm
import tifffile

# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ sys.path ä¸­
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from importlib import import_module

# æ¨¡å‹
model_mod = import_module("20_src.20_model.dual_unet")
DualHeadResUNet3D = model_mod.DualHeadResUNet3D

# æŸå¤±å‡½æ•°
loss_mod = import_module("20_src.20_model.chimera_loss")
ChimeraLoss = loss_mod.ChimeraLoss

# æ•°æ®é›†
dataset_mod = import_module("20_src.20_data.dataset")
VesuviusTrainDataset = dataset_mod.VesuviusTrainDataset

# å˜æ¢
transforms_mod = import_module("20_src.20_data.transforms")
RandomCrop3D = transforms_mod.RandomCrop3D
RandomFlipRotate3D = transforms_mod.RandomFlipRotate3D
Compose3D = transforms_mod.Compose3D


# ===== å·¥å…·å‡½æ•° =====

def get_gpu_stats():
    """è·å– GPU æ˜¾å­˜ä½¿ç”¨ä¿¡æ¯"""
    if not torch.cuda.is_available():
        return "CPU"
    allocated = torch.cuda.memory_allocated() / 1024**3
    reserved = torch.cuda.memory_reserved() / 1024**3
    total = torch.cuda.get_device_properties(0).total_memory / 1024**3
    return f"GPU:{reserved:.1f}G/{total:.1f}G"


def compute_dice(pred_logits, targets, threshold=0.5):
    """è®¡ç®— Dice ç³»æ•°ï¼ˆè¯„ä¼°æŒ‡æ ‡ï¼Œper-sample å¹³å‡ï¼‰"""
    pred = (torch.sigmoid(pred_logits) > threshold).float()
    smooth = 1e-6
    # per-sample è®¡ç®—ï¼Œé¿å…ç©º patch ç¨€é‡Šå…¨å±€ Dice
    pred_flat = pred.view(pred.size(0), -1)
    tgt_flat = targets.view(targets.size(0), -1)
    intersection = (pred_flat * tgt_flat).sum(dim=1)
    dice = (2.0 * intersection + smooth) / (pred_flat.sum(dim=1) + tgt_flat.sum(dim=1) + smooth)
    return dice.mean().item()


def format_time(seconds):
    """æ ¼å¼åŒ–æ—¶é—´"""
    td = timedelta(seconds=int(seconds))
    return str(td)


# ===== å¯è§†åŒ–å·¥å…· =====

def save_epoch_visualization(
    model, val_loader, device, run_dir, epoch, criterion
):
    """
    æ¯ä¸ª epoch ç»“æŸåä¿å­˜å¯è§†åŒ–å¯¹æ¯”ï¼š
    1. PNG å¯¹æ¯”å›¾ï¼šä¸­é—´ slice çš„ image / GT / prediction ä¸‰åˆ—å¯¹æ¯”
    2. TIF maskï¼šé¢„æµ‹ç»“æœ 3D volume
    """
    model.eval()

    # ä»éªŒè¯é›†å¯»æ‰¾ä¸€ä¸ªåŒ…å«æ­£æ ·æœ¬çš„ batch è¿›è¡Œå¯è§†åŒ–
    target_images = None
    target_labels = None
    
    try:
        # è¦æ±‚ä¸­é—´åˆ‡ç‰‡ä¸Šæœ‰è¶³å¤Ÿçš„ GT åƒç´ ï¼Œå¦åˆ™å¯è§†åŒ–çœ‹ä¸åˆ°ä»»ä½•ä¸œè¥¿
        for images, labels in val_loader:
            mid_z = images.shape[2] // 2
            mid_slice_gt = labels[:, :, mid_z, :, :].sum()
            if mid_slice_gt > 50:  # è‡³å°‘ 50 ä¸ªæ­£æ ·æœ¬åƒç´ 
                target_images = images
                target_labels = labels
                break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼ˆæå…¶ç½•è§ï¼‰ï¼Œå°±é€€å›åˆ°ç¬¬ä¸€ä¸ª batch
        if target_images is None:
            target_images, target_labels = next(iter(val_loader))
            
    except StopIteration:
        return

    images = target_images.to(device)
    labels = target_labels.to(device)

    with torch.no_grad():
        with torch.amp.autocast('cuda', enabled=(device.type == 'cuda')):
            seg_logits, normals = model(images)
    
    # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
    pred_prob = torch.sigmoid(seg_logits[0, 0]).cpu().numpy()    # (D, H, W)
    pred_mask = (pred_prob > 0.5).astype(np.uint8)               # äºŒå€¼ mask
    gt_mask = labels[0, 0].cpu().numpy()                          # (D, H, W)
    img_vol = images[0, 0].cpu().numpy()                          # (D, H, W)

    D, H, W = img_vol.shape

    # === 1. ä¿å­˜ TIF mask ===
    tif_dir = run_dir / "epoch_masks"
    tif_dir.mkdir(exist_ok=True)
    tif_path = tif_dir / f"epoch{epoch+1:03d}_pred_mask.tif"
    tifffile.imwrite(str(tif_path), pred_mask)

    # === 2. ä¿å­˜ PNG å¯¹æ¯”å›¾ ===
    try:
        import matplotlib
        matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
        import matplotlib.pyplot as plt

        vis_dir = run_dir / "epoch_vis"
        vis_dir.mkdir(exist_ok=True)

        # å– 3 ä¸ªæ­£äº¤ sliceï¼ˆä¸­é—´ä½ç½®ï¼‰
        slices = {
            'Axial (z-mid)': (img_vol[D//2], gt_mask[D//2], pred_prob[D//2], pred_mask[D//2]),
            'Coronal (y-mid)': (img_vol[:, H//2], gt_mask[:, H//2], pred_prob[:, H//2], pred_mask[:, H//2]),
            'Sagittal (x-mid)': (img_vol[:, :, W//2], gt_mask[:, :, W//2], pred_prob[:, :, W//2], pred_mask[:, :, W//2]),
        }

        fig, axes = plt.subplots(3, 4, figsize=(16, 12))
        fig.suptitle(f'Epoch {epoch+1} | Dice: {compute_dice(seg_logits[0:1], labels[0:1]):.4f}',
                     fontsize=16, fontweight='bold')

        for row_idx, (plane_name, (img_s, gt_s, prob_s, mask_s)) in enumerate(slices.items()):
            # åˆ— 1: Input CT
            axes[row_idx, 0].imshow(img_s, cmap='gray', vmin=0, vmax=1)
            axes[row_idx, 0].set_title(f'{plane_name}\nInput CT')
            axes[row_idx, 0].axis('off')

            # åˆ— 2: Ground Truth
            axes[row_idx, 1].imshow(gt_s, cmap='Reds', vmin=0, vmax=1, alpha=0.8)
            axes[row_idx, 1].set_title('Ground Truth')
            axes[row_idx, 1].axis('off')

            # åˆ— 3: Prediction (æ¦‚ç‡å›¾)
            axes[row_idx, 2].imshow(prob_s, cmap='hot', vmin=0, vmax=1)
            axes[row_idx, 2].set_title('Pred Prob')
            axes[row_idx, 2].axis('off')

            # åˆ— 4: Overlay (CT + Prediction å åŠ )
            axes[row_idx, 3].imshow(img_s, cmap='gray', vmin=0, vmax=1)
            axes[row_idx, 3].imshow(mask_s, cmap='Reds', alpha=0.4)
            axes[row_idx, 3].set_title('Overlay')
            axes[row_idx, 3].axis('off')

        plt.tight_layout()
        png_path = vis_dir / f"epoch{epoch+1:03d}_comparison.png"
        plt.savefig(str(png_path), dpi=120, bbox_inches='tight')
        plt.close(fig)

        print(f"  ğŸ“¸ 2D å¯¹æ¯”: {png_path.name} | ğŸ—‚ï¸ Mask: {tif_path.name}")

    except ImportError:
        print(f"  ğŸ—‚ï¸ Mask TIF: {tif_path.name} (matplotlib ä¸å¯ç”¨ï¼Œè·³è¿‡ PNG)")

    # === 3. å¯¼å‡ºç‹¬ç«‹ 3D é¢„è§ˆæ–‡ä»¶ (åŒå‡»ç›´æ¥æ‰“å¼€) ===
    try:
        import pyvista as pv

        vis_dir = run_dir / "epoch_vis"
        vis_dir.mkdir(exist_ok=True)

        if pred_mask.sum() > 0:
            # 1. åŒ…è£…ä½“ç´ å¹¶æå–å‡ ä½•ç½‘æ ¼
            grid = pv.wrap(pred_mask.astype(np.float32))
            mesh = grid.contour([0.5])

            # 2. æ„å»ºç¦»å±æ¸²æŸ“åœºæ™¯
            p = pv.Plotter(off_screen=True)
            p.add_mesh(mesh, color='red', opacity=0.8, show_edges=False)

            # å«å…¥ GT ä½œä¸ºå¹½çµè½®å»“å¯¹æ¯”
            if gt_mask.sum() > 0:
                gt_grid = pv.wrap(gt_mask.astype(np.float32))
                gt_mesh = gt_grid.contour([0.5])
                p.add_mesh(gt_mesh, color='green', opacity=0.15, show_edges=False)

            # --- æ ¸å¿ƒå¯¼å‡ºé€»è¾‘ ---

            # æ–¹æ¡ˆ A: ç‹¬ç«‹ HTML (åŒå‡»ç”¨æµè§ˆå™¨æ‰“å¼€ï¼Œçº¯å‡€åº•è‰²)
            html_path = vis_dir / f"epoch{epoch+1:03d}_pred.html"
            p.export_html(str(html_path))
            print(f"  ğŸŒ ç‹¬ç«‹ 3D HTML å·²å¯¼å‡º: {html_path.name}")

            # æ–¹æ¡ˆ B: GLTF æ¨¡å‹ (åŒå‡»ç”¨ Windows/Mac è‡ªå¸¦ 3D è½¯ä»¶æ‰“å¼€)
            gltf_path = vis_dir / f"epoch{epoch+1:03d}_pred.gltf"
            p.export_gltf(str(gltf_path))
            print(f"  ğŸ§Š ç‹¬ç«‹ 3D GLTF å·²å¯¼å‡º: {gltf_path.name}")

            p.close()
        else:
            print(f"  âš ï¸ Epoch {epoch+1} é¢„æµ‹ä¸ºå…¨ 0ï¼Œæ— ç‰©ç†å®ä½“å¯å¯¼å‡ºã€‚")

    except Exception as e:
        print(f"  âš ï¸ 3D å¯¼å‡ºå¤±è´¥: {e}")



# ===== è®­ç»ƒå¾ªç¯ =====

def train_one_epoch(
    model, dataloader, criterion, optimizer, scaler, device, epoch, total_epochs
):
    """è®­ç»ƒä¸€ä¸ª epochï¼ˆå¸¦ tqdm è¿›åº¦æ¡ï¼‰"""
    model.train()

    total_loss = 0.0
    total_dice_loss = 0.0
    total_bce_loss = 0.0
    total_normal_loss = 0.0
    total_dice_score = 0.0
    num_batches = 0

    pbar = tqdm(
        dataloader,
        desc=f"Train E{epoch+1}/{total_epochs}",
        ncols=120,
        leave=True,
    )

    for images, labels in pbar:
        images = images.to(device, non_blocking=True)  # (B, 1, D, H, W)
        labels = labels.to(device, non_blocking=True)  # (B, 1, D, H, W)

        optimizer.zero_grad(set_to_none=True)

        # AMP å‰å‘ä¼ æ’­
        with torch.amp.autocast('cuda', enabled=(device.type == 'cuda')):
            seg_logits, normals = model(images)
            loss_total, loss_tversky, loss_bce, loss_normal = criterion(seg_logits, normals, labels)

        # AMP åå‘ä¼ æ’­
        if device.type == 'cuda':
            scaler.scale(loss_total).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            loss_total.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

        # ç»Ÿè®¡
        dice_score = compute_dice(seg_logits.detach(), labels)
        total_loss += loss_total.item()
        total_dice_loss += loss_tversky.item()
        total_bce_loss += loss_bce.item()
        total_normal_loss += loss_normal.item()
        total_dice_score += dice_score
        num_batches += 1

        # Debug "Playing Dead"
        if num_batches % 50 == 0:
            pred_sum = (torch.sigmoid(seg_logits) > 0.5).float().sum()
            target_sum = labels.sum()
            print(f"\n[DEBUG] Batch {num_batches}: Pred_Pixels={pred_sum.item()}, GT_Pixels={target_sum.item()}")

        # æ›´æ–° tqdm
        avg_loss = total_loss / num_batches
        avg_dice = total_dice_score / num_batches
        pbar.set_postfix({
            'loss': f'{avg_loss:.2f}',
            'tvsk': f'{total_dice_loss/num_batches:.2f}',
            'dice': f'{avg_dice:.2f}',
            'norm': f'{total_normal_loss/num_batches:.2f}',
            'gpu': get_gpu_stats(),
        })

    pbar.close()

    # epoch ç»Ÿè®¡
    avg_loss = total_loss / max(num_batches, 1)
    avg_tversky_loss = total_dice_loss / max(num_batches, 1)
    avg_bce_loss = total_bce_loss / max(num_batches, 1)
    avg_normal_loss = total_normal_loss / max(num_batches, 1)
    avg_dice_score = total_dice_score / max(num_batches, 1)

    return {
        "loss": avg_loss,
        "tversky_loss": avg_tversky_loss,
        "bce_loss": avg_bce_loss,
        "normal_loss": avg_normal_loss,
        "dice_score": avg_dice_score,
    }


@torch.no_grad()
def validate(model, dataloader, criterion, device, epoch, total_epochs):
    """éªŒè¯ä¸€ä¸ª epochï¼ˆå¸¦ tqdmï¼‰"""
    model.eval()

    total_loss = 0.0
    total_dice_score = 0.0
    num_batches = 0

    pbar = tqdm(
        dataloader,
        desc=f"Val   E{epoch+1}/{total_epochs}",
        ncols=120,
        leave=True,
    )

    for images, labels in pbar:
        images = images.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)

        with torch.amp.autocast('cuda', enabled=(device.type == 'cuda')):
            seg_logits, normals = model(images)
            loss_total, loss_tversky, loss_bce, loss_normal = criterion(seg_logits, normals, labels)

        dice_score = compute_dice(seg_logits, labels)
        total_loss += loss_total.item()
        total_dice_score += dice_score
        num_batches += 1

        pbar.set_postfix({
            'val_loss': f'{total_loss/num_batches:.4f}',
            'val_dice': f'{total_dice_score/num_batches:.4f}',
        })

    pbar.close()

    avg_loss = total_loss / max(num_batches, 1)
    avg_dice = total_dice_score / max(num_batches, 1)

    return {"val_loss": avg_loss, "val_dice": avg_dice}


# ===== ä¸»è®­ç»ƒå‡½æ•° =====

def main(args):
    # è®¾å¤‡
    if args.device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)

    # è¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = Path(args.output_dir) / f"train_{timestamp}"
    run_dir.mkdir(parents=True, exist_ok=True)

    print(f"\n{'='*70}")
    print(f"  ğŸ”¥ Hybrid Chimera è®­ç»ƒå¼•æ“")
    print(f"  è®¾å¤‡: {device}")
    print(f"  Patch å¤§å°: {args.crop_size}Â³")
    print(f"  Batch Size: {args.batch_size}")
    print(f"  Learning Rate: {args.lr}")
    print(f"  Epochs: {args.epochs}")
    print(f"  Î»_normal: {args.lambda_normal}")
    print(f"  è¾“å‡º: {run_dir}")
    print(f"{'='*70}\n")

    # ===== æ•°æ® =====
    # å¢å¼ºå˜æ¢ï¼šä»… FlipRotateï¼ŒCrop å·²å†…ç½®åˆ° Dataset çš„ memmap __getitem__
    aug_transform = RandomFlipRotate3D(flip_prob=0.5, rotate_prob=0.5)

    full_dataset = VesuviusTrainDataset(
        image_dir=args.image_dir,
        label_dir=args.label_dir,
        crop_size=args.crop_size,
        transform=aug_transform,
        samples_per_volume=args.samples_per_volume,
        cache_size=args.cache_size,
        max_files=args.max_chunks,
    )


    # æŒ‰ 8:2 æ‹†åˆ† train/val
    total_len = len(full_dataset)
    val_len = max(1, int(total_len * 0.2))
    train_len = total_len - val_len

    train_dataset, val_dataset = random_split(
        full_dataset, [train_len, val_len],
        generator=torch.Generator().manual_seed(42),
    )

    print(f"[Data] Train: {train_len} samples, Val: {val_len} samples")

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        pin_memory=(device.type == "cuda"),
        drop_last=True,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=(device.type == "cuda"),
    )

    # ===== æ¨¡å‹ =====
    model = DualHeadResUNet3D(in_channels=1, n_filters=args.n_filters).to(device)
    params = sum(p.numel() for p in model.parameters())
    print(f"[Model] å‚æ•°é‡: {params:,}")

    # æ¢å¤ checkpoint
    start_epoch = 0
    best_dice = 0.0

    if args.resume and os.path.exists(args.resume):
        ckpt = torch.load(args.resume, map_location=device, weights_only=False)
        if isinstance(ckpt, dict) and "model_state_dict" in ckpt:
            model.load_state_dict(ckpt["model_state_dict"])
            start_epoch = ckpt.get("epoch", 0)
            best_dice = ckpt.get("best_dice", 0.0)
            print(f"[Resume] ä» epoch {start_epoch} æ¢å¤, best_dice={best_dice:.4f}")
        else:
            if any(k.startswith("model.") for k in ckpt.keys()):
                ckpt = {k.replace("model.", ""): v for k, v in ckpt.items()}
            model.load_state_dict(ckpt, strict=False)
            print(f"[Resume] åŠ è½½æƒé‡ï¼ˆæ—  epoch ä¿¡æ¯ï¼‰")

    # ===== ä¼˜åŒ–å™¨ & è°ƒåº¦å™¨ =====
    criterion = ChimeraLoss(
        lambda_normal=args.lambda_normal,
        lambda_bce=args.lambda_bce,
        pos_weight=args.pos_weight,
        tversky_alpha=args.tversky_alpha,
        tversky_beta=args.tversky_beta,
    ).to(device)
    print(f"[Loss] Tversky(a={args.tversky_alpha}, b={args.tversky_beta}) + BCE(pw={args.pos_weight}, lam={args.lambda_bce}) + Normal(lam={args.lambda_normal})")

    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=args.weight_decay,
    )

    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=args.epochs,
        eta_min=args.lr * 0.01,
    )

    scaler = torch.amp.GradScaler('cuda', enabled=(device.type == 'cuda'))

    # ===== è®­ç»ƒå¾ªç¯ =====
    print(f"\n{'='*70}")
    print(f"  å¼€å§‹è®­ç»ƒ (epoch {start_epoch+1} â†’ {args.epochs})")
    print(f"{'='*70}\n")

    history = []
    t_total_start = time.time()

    for epoch in range(start_epoch, args.epochs):
        t_ep_start = time.time()
        lr_now = optimizer.param_groups[0]["lr"]

        print(f"\n--- Epoch {epoch+1}/{args.epochs} | LR: {lr_now:.6f} ---")

        # è®­ç»ƒ
        train_metrics = train_one_epoch(
            model, train_loader, criterion, optimizer, scaler,
            device, epoch, args.epochs,
        )

        # éªŒè¯
        val_metrics = validate(model, val_loader, criterion, device, epoch, args.epochs)

        # è°ƒåº¦å™¨
        scheduler.step()

        # æ¯ epoch å¯è§†åŒ–è¾“å‡º
        save_epoch_visualization(
            model, val_loader, device, run_dir, epoch, criterion
        )

        # è®°å½•
        ep_time = time.time() - t_ep_start
        epoch_info = {
            "epoch": epoch + 1,
            "lr": lr_now,
            "time": ep_time,
            **train_metrics,
            **val_metrics,
        }
        history.append(epoch_info)

        # æ‰“å° epoch æ€»ç»“
        print(
            f"\n  ğŸ“Š Epoch {epoch+1} æ€»ç»“:"
            f"\n     Train - Loss: {train_metrics['loss']:.4f} | "
            f"Tversky: {train_metrics['tversky_loss']:.4f} | "
            f"Dice: {train_metrics['dice_score']:.4f} | "
            f"Normal: {train_metrics['normal_loss']:.4f}"
            f"\n     Val   - Loss: {val_metrics['val_loss']:.4f} | "
            f"Dice: {val_metrics['val_dice']:.4f}"
            f"\n     Time: {format_time(ep_time)} | LR: {lr_now:.6f}"
        )

        # ä¿å­˜ best model
        if val_metrics["val_dice"] > best_dice:
            best_dice = val_metrics["val_dice"]
            best_path = run_dir / "best_model.pth"
            torch.save({
                "epoch": epoch + 1,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "best_dice": best_dice,
                "args": vars(args),
            }, str(best_path))
            print(f"  ğŸ† New Best Dice: {best_dice:.4f} â†’ {best_path.name}")

        # å®šæœŸä¿å­˜ checkpoint
        if (epoch + 1) % args.save_every == 0:
            ckpt_path = run_dir / f"checkpoint_epoch{epoch+1:03d}.pth"
            torch.save({
                "epoch": epoch + 1,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "best_dice": best_dice,
                "args": vars(args),
            }, str(ckpt_path))
            print(f"  ğŸ’¾ Checkpoint: {ckpt_path.name}")

    # ===== æœ€ç»ˆæŠ¥å‘Š =====
    total_time = time.time() - t_total_start

    print(f"\n{'='*70}")
    print(f"  è®­ç»ƒå®Œæˆ!")
    print(f"  æ€» Epochs: {args.epochs - start_epoch}")
    print(f"  æ€»è€—æ—¶: {format_time(total_time)}")
    print(f"  æœ€ä½³ Val Dice: {best_dice:.4f}")
    print(f"  è¾“å‡ºç›®å½•: {run_dir}")
    print(f"{'='*70}\n")

    # ä¿å­˜è®­ç»ƒå†å²
    history_path = run_dir / "training_history.json"
    with open(str(history_path), "w", encoding="utf-8") as f:
        json.dump(history, f, indent=2, ensure_ascii=False)
    print(f"  ğŸ“ˆ è®­ç»ƒå†å²: {history_path.name}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Hybrid Chimera è®­ç»ƒå¼•æ“")

    # æ•°æ®å‚æ•°
    parser.add_argument("--image_dir", type=str,
                        default="data/vesuvius-challenge-surface-detection/train_images")
    parser.add_argument("--label_dir", type=str,
                        default="data/vesuvius-challenge-surface-detection/train_labels")
    parser.add_argument("--max_chunks", type=int, default=None,
                        help="æœ€å¤šä½¿ç”¨å¤šå°‘ä¸ª chunkï¼ˆè°ƒè¯•ç”¨ï¼‰")
    parser.add_argument("--samples_per_volume", type=int, default=4,
                        help="æ¯ä¸ªä½“ç§¯æ¯ epoch é‡‡é›†å‡ ä¸ª patch")
    parser.add_argument("--cache_size", type=int, default=8,
                        help="LRU ç¼“å­˜ä½“ç§¯æ•°é‡")

    # è®­ç»ƒå‚æ•°
    parser.add_argument("--epochs", type=int, default=50)
    parser.add_argument("--batch_size", type=int, default=8)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--weight_decay", type=float, default=1e-4)
    parser.add_argument("--crop_size", type=int, default=64,
                        help="Random Crop 3D å°ºå¯¸")
    parser.add_argument("--lambda_normal", type=float, default=0.0,
                        help="æ³•çº¿æŸå¤±æƒé‡ (åˆ†å‰²æœªæ”¶æ•›å‰å…ˆå…³é—­)")
    parser.add_argument("--lambda_bce", type=float, default=0.3,
                        help="BCE æŸå¤±æƒé‡")
    parser.add_argument("--pos_weight", type=float, default=1.0,
                        help="BCE æ­£æ ·æœ¬æƒé‡ (1.0=æ ‡å‡† BCE)")
    parser.add_argument("--tversky_alpha", type=float, default=0.3,
                        help="Tversky FP æƒ©ç½šç³»æ•°")
    parser.add_argument("--tversky_beta", type=float, default=0.7,
                        help="Tversky FN æƒ©ç½šç³»æ•° (é¼“åŠ±å¬å›)")
    parser.add_argument("--n_filters", type=int, default=32,
                        help="æ¨¡å‹åŸºç¡€æ»¤æ³¢å™¨æ•°")
    parser.add_argument("--num_workers", type=int, default=0,
                        help="DataLoader å·¥ä½œè¿›ç¨‹æ•°")

    # ä¿å­˜å‚æ•°
    parser.add_argument("--output_dir", type=str, default="20_src/output")
    parser.add_argument("--save_every", type=int, default=10,
                        help="æ¯å‡ ä¸ª epoch ä¿å­˜ checkpoint")
    parser.add_argument("--resume", type=str, default=None,
                        help="æ¢å¤è®­ç»ƒçš„ checkpoint è·¯å¾„")

    # è®¾å¤‡
    parser.add_argument("--device", type=str, default="auto")

    args = parser.parse_args()
    main(args)
