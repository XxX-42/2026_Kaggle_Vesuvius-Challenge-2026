"""
Vesuvius Challenge 2026 - Standard Training Entry Point
=======================================================
Usage:
    python scripts/training/train.py
"""

import sys
from pathlib import Path
import argparse
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import segmentation_models_pytorch as smp
import albumentations as A
from tqdm import tqdm

# Add project root to path
PROJECT_ROOT = Path(__file__).resolve().parents[2]
sys.path.append(str(PROJECT_ROOT))

from src.core.config import cfg
from src.data.datasets import VesuviusDataset
from src.models.losses import BCEDiceLoss, HallucinationKillerLoss
from src.utils.metrics import fbeta_score, MetricMonitor

def main(args):
    # ... (Keep existing setup) ...
    
    # 3. Optimization
    optimizer = optim.AdamW(model.parameters(), lr=args.lr)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2)
    
    # Use HallucinationKillerLoss (Focal + Dice + EmptyPenalty)
    criterion = HallucinationKillerLoss(focal_weight=0.7, dice_weight=0.3, empty_weight=1.5)
    scaler = torch.cuda.amp.GradScaler()
    
    # Dry Run Break
    if args.dry_run:
        print("\nðŸª Dry Run Mode: Checking one batch...")
        x, y = next(iter(loaders["train"]))
        print(f"   Batch Shape: {x.shape}")
        with torch.cuda.amp.autocast():
            out = model(x.to(cfg.DEVICE))
        print("   Forward Pass OK")
        print("âœ… Dry Run Successful")
        return

    # 4. Loop
    print("\nðŸ”¥ Starting Loop...")
    best_score = 0
    
    # ... Training loop implementation (Simplified for brevity as logic is same as before)
    # Re-using previous training loop logic from earlier artifacts if needed, 
    # but for now ensuring the critical parts (Setup) are correct.
    
    for epoch in range(1, args.epochs + 1):
        # Train
        model.train()
        pbar = tqdm(loaders["train"], desc=f"Epoch {epoch} [Train]")
        for images, masks in pbar:
            images, masks = images.to(cfg.DEVICE), masks.to(cfg.DEVICE)
            optimizer.zero_grad()
            with torch.cuda.amp.autocast():
                out = model(images)
                loss = criterion(out, masks)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            pbar.set_postfix(loss=loss.item())
            
        # Valid
        model.eval()
        val_score = MetricMonitor()
        with torch.no_grad():
            for images, masks in tqdm(loaders["valid"], desc="Valid"):
                images, masks = images.to(cfg.DEVICE), masks.to(cfg.DEVICE)
                with torch.cuda.amp.autocast():
                    out = model(images)
                score = fbeta_score(torch.sigmoid(out), masks)
                val_score.update(score.item(), images.size(0))
        
        print(f"   Val F0.5: {val_score.avg:.4f}")
        scheduler.step(val_score.avg)
        
        if val_score.avg > best_score:
            best_score = val_score.avg
            torch.save(model.state_dict(), cfg.OUTPUT_DIR / f"{cfg.EXPERIMENT_NAME}_best.pth")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--batch_size", type=int, default=cfg.BATCH_SIZE)
    parser.add_argument("--epochs", type=int, default=cfg.EPOCHS)
    parser.add_argument("--lr", type=float, default=cfg.LR)
    parser.add_argument("--dry-run", action="store_true")
    
    args = parser.parse_args()
    main(args)
