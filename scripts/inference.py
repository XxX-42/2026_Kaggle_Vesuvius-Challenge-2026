"""
Vesuvius Inference Script (Stage 4)

ä¸»æ¨ç†å…¥å£ç‚¹ã€‚è‡ªåŠ¨åŠ è½½æœ€ä½³ Checkpointï¼Œæ‰§è¡Œæ»‘çª—é¢„æµ‹ï¼Œç”Ÿæˆå¯è§†åŒ–å’Œæäº¤æ–‡ä»¶ã€‚

Usage:
    python scripts/inference.py
    
    # æ‰‹åŠ¨æŒ‡å®š Checkpoint:
    python scripts/inference.py --checkpoint checkpoints/MiniUNETR_20260208_001652/best_model.pth

Output:
    - output/inference/prediction_raw.png: æ¦‚ç‡å›¾
    - output/inference/overlay.png: é¢„æµ‹å åŠ å¯è§†åŒ–
    - output/inference/submission.csv: RLE ç¼–ç æäº¤æ–‡ä»¶
"""

import sys
import os
import argparse
import yaml
import logging
from pathlib import Path
from datetime import datetime
import numpy as np
from PIL import Image
import torch
import cv2
from tqdm import tqdm
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt

# ç¡®ä¿ src åœ¨è·¯å¾„ä¸­
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.models.mini_unetr import MiniUNETR
from src.inference.predictor import VesuviusPredictor

# ============================================================================
# é…ç½®
# ============================================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def find_best_checkpoint(checkpoint_dir="checkpoints"):
    """
    è‡ªåŠ¨æ‰«æ checkpoints ç›®å½•ï¼Œæ‰¾åˆ°æœ€æ–°çš„ best_model.pth
    
    æœç´¢ç­–ç•¥:
    1. ä¼˜å…ˆæŸ¥æ‰¾åŒ…å« 'BEST' çš„ç›®å½•
    2. å…¶æ¬¡æŒ‰æ—¶é—´æˆ³æ’åºï¼Œé€‰æ‹©æœ€æ–°çš„
    3. åœ¨ç›®å½•ä¸­æŸ¥æ‰¾ best_model.pth
    
    Returns:
        str: æœ€ä½³ Checkpoint è·¯å¾„
        
    å¦‚æœè‡ªåŠ¨åŠ è½½å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®š:
        python scripts/inference.py --checkpoint <è·¯å¾„>
    """
    checkpoint_path = Path(checkpoint_dir)
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"Checkpoint directory not found: {checkpoint_dir}")
    
    # è·å–æ‰€æœ‰å­ç›®å½•
    subdirs = [d for d in checkpoint_path.iterdir() if d.is_dir()]
    
    if not subdirs:
        raise FileNotFoundError(f"No checkpoint subdirectories found in {checkpoint_dir}")
    
    # ç­–ç•¥ 1: æŸ¥æ‰¾åŒ…å« 'BEST' çš„ç›®å½•
    best_dirs = [d for d in subdirs if 'BEST' in d.name.upper()]
    if best_dirs:
        best_dir = sorted(best_dirs)[-1]  # å–æœ€æ–°çš„
        best_path = best_dir / "best_model.pth"
        if best_path.exists():
            logger.info(f"Found BEST checkpoint: {best_path}")
            return str(best_path)
    
    # ç­–ç•¥ 2: æŒ‰æ—¶é—´æˆ³æ’åº (å‡è®¾ç›®å½•ååŒ…å«æ—¶é—´æˆ³ YYYYMMDD_HHMMSS)
    dated_dirs = []
    for d in subdirs:
        parts = d.name.split('_')
        if len(parts) >= 3:
            try:
                # å°è¯•è§£ææ—¶é—´æˆ³
                date_str = '_'.join(parts[-2:])
                datetime.strptime(date_str, "%Y%m%d_%H%M%S")
                dated_dirs.append(d)
            except ValueError:
                continue
    
    if dated_dirs:
        # æŒ‰ç›®å½•åæ’åº (æ—¶é—´æˆ³åœ¨åç§°ä¸­)
        dated_dirs.sort(key=lambda x: x.name, reverse=True)
        
        for d in dated_dirs:
            best_path = d / "best_model.pth"
            if best_path.exists():
                logger.info(f"Found latest checkpoint: {best_path}")
                return str(best_path)
            
            # å¦‚æœæ²¡æœ‰ best_model.pthï¼Œå°è¯• last_model.pth
            last_path = d / "last_model.pth"
            if last_path.exists():
                logger.info(f"Found latest checkpoint (last): {last_path}")
                return str(last_path)
    
    # ç­–ç•¥ 3: éå†æ‰€æœ‰ç›®å½•æ‰¾ best_model.pth
    for d in subdirs:
        best_path = d / "best_model.pth"
        if best_path.exists():
            logger.info(f"Found checkpoint: {best_path}")
            return str(best_path)
    
    raise FileNotFoundError(
        f"No checkpoint found in {checkpoint_dir}. "
        "Please specify manually with --checkpoint <path>"
    )


def load_model(checkpoint_path, config, device='cuda'):
    """
    åŠ è½½æ¨¡å‹å’Œæƒé‡
    
    Args:
        checkpoint_path: Checkpoint æ–‡ä»¶è·¯å¾„
        config: æ¨¡å‹é…ç½®å­—å…¸
        device: è®¡ç®—è®¾å¤‡
        
    Returns:
        model: åŠ è½½å¥½æƒé‡çš„æ¨¡å‹
    """
    logger.info(f"Loading model from: {checkpoint_path}")
    
    model_cfg = config['model']
    model = MiniUNETR(
        in_channels=model_cfg['in_channels'],
        out_channels=model_cfg['out_channels'],
        feature_size=model_cfg['feature_size'],
        hidden_size=model_cfg['hidden_size'],
        num_heads=model_cfg.get('num_heads', 8)
    )
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # å¤„ç†ä¸åŒçš„ Checkpoint æ ¼å¼
    if 'model_state_dict' in checkpoint:
        state_dict = checkpoint['model_state_dict']
    elif 'state_dict' in checkpoint:
        state_dict = checkpoint['state_dict']
    else:
        state_dict = checkpoint
    
    model.load_state_dict(state_dict)
    model.to(device)
    model.eval()
    
    # æ‰“å° Checkpoint ä¿¡æ¯
    if 'metrics' in checkpoint:
        metrics = checkpoint['metrics']
        logger.info(f"Checkpoint Metrics: {metrics}")
    if 'epoch' in checkpoint:
        logger.info(f"Checkpoint Epoch: {checkpoint['epoch']}")
    
    return model


def apply_morphology(binary_mask, config):
    """
    å¯¹äºŒå€¼åŒ–æ©ç åº”ç”¨å½¢æ€å­¦åå¤„ç†
    
    Args:
        binary_mask: äºŒå€¼åŒ–æ©ç  (H, W)ï¼Œdtype=np.uint8ï¼Œå€¼ä¸º 0 æˆ– 255
        config: é…ç½®å­—å…¸ï¼ŒåŒ…å« inference.morphology è®¾ç½®
        
    Returns:
        np.ndarray: å¤„ç†åçš„äºŒå€¼åŒ–æ©ç 
        
    æ”¯æŒçš„æ“ä½œï¼š
        - opening: å…ˆè…èš€åè†¨èƒ€ï¼Œå‰”é™¤ç»†å°å­¤ç«‹å™ªç‚¹
        - closing: å…ˆè†¨èƒ€åè…èš€ï¼Œå¡«å……ç»†å°ç©ºæ´
        - dilate: è†¨èƒ€æ“ä½œ
        - erode: è…èš€æ“ä½œ
    """
    morph_cfg = config.get('inference', {}).get('morphology', {})
    
    if not morph_cfg.get('enabled', False):
        return binary_mask
    
    operation = morph_cfg.get('operation', 'opening')
    kernel_size = morph_cfg.get('kernel_size', 3)
    
    # åˆ›å»ºç»“æ„å…ƒç´  (æ¤­åœ†å½¢é€šå¸¸æ•ˆæœæ›´å¥½)
    kernel = cv2.getStructuringElement(
        cv2.MORPH_ELLIPSE, 
        (kernel_size, kernel_size)
    )
    
    logger.info(f"Applying morphology: {operation} with kernel {kernel_size}x{kernel_size}")
    
    if operation == 'opening':
        # Opening = Erosion followed by Dilation
        # æ•ˆæœï¼šå‰”é™¤ç»†å°çš„å­¤ç«‹å™ªç‚¹ï¼Œä¿æŒè¾ƒå¤§åŒºåŸŸçš„å½¢çŠ¶
        result = cv2.morphologyEx(binary_mask, cv2.MORPH_OPEN, kernel)
    elif operation == 'closing':
        # Closing = Dilation followed by Erosion
        # æ•ˆæœï¼šå¡«å……ç»†å°çš„ç©ºæ´ï¼Œè¿æ¥ç›¸é‚»åŒºåŸŸ
        result = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel)
    elif operation == 'dilate':
        result = cv2.dilate(binary_mask, kernel)
    elif operation == 'erode':
        result = cv2.erode(binary_mask, kernel)
    else:
        logger.warning(f"Unknown morphology operation: {operation}, skipping...")
        result = binary_mask
    
    return result


def rle_encode(mask, threshold=0.5):
    """
    Run-Length ç¼–ç  (ç”¨äº Kaggle æäº¤)
    
    Args:
        mask: 2D NumPy æ•°ç»„ (æ¦‚ç‡å›¾)
        threshold: äºŒå€¼åŒ–é˜ˆå€¼
        
    Returns:
        str: RLE ç¼–ç å­—ç¬¦ä¸²
    """
    binary = (mask > threshold).astype(np.uint8)
    pixels = binary.flatten()
    
    # æ·»åŠ é¦–å°¾ 0 ä»¥å¤„ç†è¾¹ç•Œ
    pixels = np.concatenate([[0], pixels, [0]])
    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1
    runs[1::2] -= runs[::2]
    
    return ' '.join(str(x) for x in runs)


def rle_encode_binary(binary_mask):
    """
    å¯¹å·²äºŒå€¼åŒ–çš„æ©ç è¿›è¡Œ Run-Length ç¼–ç 
    
    Args:
        binary_mask: 2D NumPy æ•°ç»„ (äºŒå€¼åŒ–æ©ç ï¼Œå€¼ä¸º 0 æˆ– 255)
        
    Returns:
        str: RLE ç¼–ç å­—ç¬¦ä¸²
    """
    # å°† 0/255 è½¬æ¢ä¸º 0/1
    pixels = (binary_mask > 0).astype(np.uint8).flatten()
    
    # æ·»åŠ é¦–å°¾ 0 ä»¥å¤„ç†è¾¹ç•Œ
    pixels = np.concatenate([[0], pixels, [0]])
    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1
    runs[1::2] -= runs[::2]
    
    return ' '.join(str(x) for x in runs)


def save_visualization(prediction, fragment_path, output_dir, threshold=0.5, config=None):
    """
    ä¿å­˜å¯è§†åŒ–ç»“æœ
    
    Args:
        prediction: æ¦‚ç‡å›¾ (H, W)
        fragment_path: Fragment è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        threshold: äºŒå€¼åŒ–é˜ˆå€¼
        config: é…ç½®å­—å…¸ï¼ˆç”¨äºå½¢æ€å­¦åå¤„ç†ï¼‰
        
    Returns:
        np.ndarray: å¤„ç†åçš„äºŒå€¼åŒ–æ©ç  (H, W)ï¼Œå€¼ä¸º 0 æˆ– 255
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    H, W = prediction.shape
    
    # 1. ä¿å­˜åŸå§‹æ¦‚ç‡å›¾
    logger.info("Saving prediction_raw.png...")
    pred_img = (prediction * 255).astype(np.uint8)
    Image.fromarray(pred_img).save(output_dir / "prediction_raw.png")
    
    # 2. äºŒå€¼åŒ–
    logger.info(f"Thresholding at {threshold}...")
    binary = (prediction > threshold).astype(np.uint8) * 255
    
    # 2.5 ä¿å­˜åŸå§‹äºŒå€¼åŒ–ï¼ˆå½¢æ€å­¦å¤„ç†å‰ï¼‰
    Image.fromarray(binary).save(output_dir / "prediction_binary_raw.png")
    
    # 3. åº”ç”¨å½¢æ€å­¦åå¤„ç†
    if config is not None:
        binary = apply_morphology(binary, config)
    
    # 4. ä¿å­˜å¤„ç†åçš„äºŒå€¼åŒ–é¢„æµ‹
    logger.info("Saving prediction_binary.png (after morphology)...")
    Image.fromarray(binary).save(output_dir / "prediction_binary.png")
    
    # 3. åˆ›å»º Overlay
    logger.info("Creating overlay...")
    
    # å°è¯•åŠ è½½ IR å›¾åƒæˆ– Mask
    ir_path = os.path.join(fragment_path, "ir.png")
    mask_path = os.path.join(fragment_path, "mask.png")
    
    if os.path.exists(ir_path):
        base_img = np.array(Image.open(ir_path).convert('L'))
        logger.info("Using ir.png as base")
    elif os.path.exists(mask_path):
        base_img = np.array(Image.open(mask_path).convert('L'))
        logger.info("Using mask.png as base")
    else:
        # ä½¿ç”¨ç°è‰²èƒŒæ™¯
        base_img = np.full((H, W), 128, dtype=np.uint8)
        logger.info("Using gray background")
    
    # ç¡®ä¿å°ºå¯¸åŒ¹é…
    if base_img.shape != (H, W):
        base_img = cv2.resize(base_img, (W, H))
    
    # åˆ›å»º RGB Overlay
    overlay = np.stack([base_img, base_img, base_img], axis=-1)
    
    # å°†é¢„æµ‹å åŠ ä¸ºçº¢è‰²é€šé“ï¼ˆä½¿ç”¨å½¢æ€å­¦å¤„ç†åçš„äºŒå€¼å›¾ï¼‰
    pred_mask = binary > 0  # ä½¿ç”¨å¤„ç†åçš„äºŒå€¼å›¾
    overlay[pred_mask, 0] = 255  # Red channel
    overlay[pred_mask, 1] = 0
    overlay[pred_mask, 2] = 0
    
    # ä¿å­˜
    Image.fromarray(overlay).save(output_dir / "overlay.png")
    logger.info(f"Saved overlay to {output_dir / 'overlay.png'}")
    
    # 4. ä¿å­˜å¸¦æœ‰åŠé€æ˜å åŠ çš„ç‰ˆæœ¬
    alpha = 0.5
    overlay_blend = base_img.astype(np.float32)
    overlay_blend = np.stack([overlay_blend, overlay_blend, overlay_blend], axis=-1)
    
    pred_color = np.zeros((H, W, 3), dtype=np.float32)
    pred_color[:, :, 0] = prediction * 255  # Red = probability
    
    blended = overlay_blend * (1 - alpha) + pred_color * alpha
    blended = np.clip(blended, 0, 255).astype(np.uint8)
    
    Image.fromarray(blended).save(output_dir / "overlay_blend.png")
    
    return binary


def main():
    parser = argparse.ArgumentParser(description="Vesuvius Inference Pipeline")
    parser.add_argument(
        "--checkpoint", 
        type=str, 
        default=None,
        help="Path to model checkpoint. If not specified, auto-detect best checkpoint."
    )
    parser.add_argument(
        "--config",
        type=str,
        default="configs/inference.yaml",
        help="Path to inference config"
    )
    parser.add_argument(
        "--fragment",
        type=str,
        default="1",
        help="Fragment ID to predict (1, 2, or 3)"
    )
    parser.add_argument(
        "--data-path",
        type=str,
        default="./data",
        help="Path to data directory"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="output/inference",
        help="Output directory"
    )
    
    args = parser.parse_args()
    
    # 1. åŠ è½½é…ç½®
    logger.info(f"Loading config from {args.config}")
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # è¡¥å……é»˜è®¤æ•°æ®é…ç½®
    if 'data' not in config:
        config['data'] = {}
    config['data'].setdefault('z_start', 29)
    config['data'].setdefault('z_end', 44)
    
    # 2. ç¡®å®š Checkpoint
    if args.checkpoint:
        checkpoint_path = args.checkpoint
    else:
        checkpoint_path = find_best_checkpoint()
    
    # 3. åŠ è½½æ¨¡å‹
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"Using device: {device}")
    model = load_model(checkpoint_path, config, device)
    
    # 4. åˆ›å»º Predictor
    predictor = VesuviusPredictor(model, config, device)
    
    # 5. æ¨ç†
    fragment_path = os.path.join(args.data_path, "native", "train", args.fragment)
    if not os.path.exists(fragment_path):
        # å°è¯•å…¶ä»–è·¯å¾„æ ¼å¼
        fragment_path = os.path.join(args.data_path, "train", args.fragment)
    
    if not os.path.exists(fragment_path):
        raise FileNotFoundError(f"Fragment not found: {fragment_path}")
    
    logger.info(f"Predicting fragment {args.fragment} at {fragment_path}")
    prediction = predictor.predict_fragment(fragment_path, args.fragment)
    
    # 6. ä¿å­˜ç»“æœ
    output_dir = Path(args.output) / f"fragment_{args.fragment}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    binary_result = save_visualization(
        prediction, 
        fragment_path, 
        output_dir, 
        threshold=config['inference']['threshold'],
        config=config
    )
    
    # 7. ç”Ÿæˆ RLE æäº¤ï¼ˆä½¿ç”¨å½¢æ€å­¦å¤„ç†åçš„äºŒå€¼å›¾ï¼‰
    logger.info("Generating submission.csv...")
    # ç›´æ¥ä½¿ç”¨å¤„ç†åçš„äºŒå€¼å›¾ï¼Œé¿å…é‡å¤äºŒå€¼åŒ–
    rle = rle_encode_binary(binary_result)
    
    submission_path = output_dir / "submission.csv"
    with open(submission_path, 'w') as f:
        f.write("Id,Predicted\n")
        f.write(f"{args.fragment},{rle}\n")
    
    logger.info(f"Saved submission to {submission_path}")
    logger.info("Inference complete!")
    
    # æ‰“å°ç»Ÿè®¡
    pred_binary = prediction > config['inference']['threshold']
    logger.info(f"Prediction Stats:")
    logger.info(f"  - Shape: {prediction.shape}")
    logger.info(f"  - Min/Max: {prediction.min():.4f} / {prediction.max():.4f}")
    logger.info(f"  - Mean: {prediction.mean():.4f}")
    logger.info(f"  - Positive Pixels: {pred_binary.sum()} ({pred_binary.mean()*100:.2f}%)")
    
    # ========================================================================
    # 8. ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒç›´æ–¹å›¾ (Stage 6 è¯Šæ–­åŠŸèƒ½)
    # ========================================================================
    logger.info("Generating probability histogram...")
    
    # åªç»Ÿè®¡æœ‰æ•ˆåŒºåŸŸçš„æ¦‚ç‡ï¼ˆæ’é™¤å…¨é»‘èƒŒæ™¯ï¼‰
    mask_path = os.path.join(fragment_path, "mask.png")
    if os.path.exists(mask_path):
        mask = np.array(Image.open(mask_path).convert('L')).astype(np.float32) / 255.0
        valid_probs = prediction[mask > 0.5]
    else:
        valid_probs = prediction.flatten()
    
    # åˆ›å»ºç›´æ–¹å›¾
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # 1. å®Œæ•´ç›´æ–¹å›¾ (0-1)
    ax = axes[0, 0]
    ax.hist(valid_probs, bins=100, range=(0, 1), color='steelblue', edgecolor='none', alpha=0.7)
    ax.axvline(x=config['inference']['threshold'], color='red', linestyle='--', linewidth=2, 
               label=f'Threshold={config["inference"]["threshold"]:.2f}')
    ax.set_xlabel('Probability', fontsize=12)
    ax.set_ylabel('Pixel Count', fontsize=12)
    ax.set_title('Full Probability Distribution', fontsize=14)
    ax.legend()
    ax.set_yscale('log')  # å¯¹æ•°åˆ»åº¦ä¾¿äºè§‚å¯Ÿ
    ax.grid(True, alpha=0.3)
    
    # 2. é«˜æ¦‚ç‡åŒºé—´ (0.4-1.0) æ”¾å¤§
    ax = axes[0, 1]
    high_probs = valid_probs[valid_probs > 0.4]
    if len(high_probs) > 0:
        ax.hist(high_probs, bins=60, range=(0.4, 1.0), color='darkgreen', edgecolor='none', alpha=0.7)
        ax.axvline(x=config['inference']['threshold'], color='red', linestyle='--', linewidth=2)
        ax.set_xlabel('Probability', fontsize=12)
        ax.set_ylabel('Pixel Count', fontsize=12)
        ax.set_title('High Probability Zone (0.4-1.0)', fontsize=14)
        ax.grid(True, alpha=0.3)
    else:
        ax.text(0.5, 0.5, 'No pixels > 0.4', ha='center', va='center', transform=ax.transAxes)
        ax.set_title('High Probability Zone (0.4-1.0)', fontsize=14)
    
    # 3. æ¦‚ç‡å¯†åº¦æ›²çº¿ (KDE è¿‘ä¼¼)
    ax = axes[1, 0]
    hist_counts, bin_edges = np.histogram(valid_probs, bins=100, range=(0, 1))
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    ax.fill_between(bin_centers, hist_counts / hist_counts.sum(), alpha=0.5, color='purple')
    ax.plot(bin_centers, hist_counts / hist_counts.sum(), color='purple', linewidth=2)
    ax.axvline(x=config['inference']['threshold'], color='red', linestyle='--', linewidth=2,
               label=f'Threshold={config["inference"]["threshold"]:.2f}')
    ax.set_xlabel('Probability', fontsize=12)
    ax.set_ylabel('Density', fontsize=12)
    ax.set_title('Probability Density Curve', fontsize=14)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    # 4. é˜ˆå€¼æ•æ„Ÿåº¦åˆ†æ
    ax = axes[1, 1]
    thresholds = np.arange(0.5, 1.0, 0.05)
    positive_ratios = []
    for t in thresholds:
        ratio = (valid_probs > t).sum() / len(valid_probs) * 100
        positive_ratios.append(ratio)
    ax.plot(thresholds, positive_ratios, 'b-o', linewidth=2, markersize=8)
    ax.axvline(x=config['inference']['threshold'], color='red', linestyle='--', linewidth=2,
               label=f'Current={config["inference"]["threshold"]:.2f}')
    ax.set_xlabel('Threshold', fontsize=12)
    ax.set_ylabel('Positive Ratio (%)', fontsize=12)
    ax.set_title('Threshold Sensitivity Analysis', fontsize=14)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.suptitle(f'Probability Distribution Analysis - Fragment {args.fragment}', fontsize=16, y=1.02)
    plt.tight_layout()
    
    hist_path = output_dir / "prob_histogram.png"
    plt.savefig(hist_path, dpi=150, bbox_inches='tight')
    plt.close()
    logger.info(f"Saved probability histogram to {hist_path}")
    
    # æ‰“å°è¯Šæ–­æ‘˜è¦
    logger.info("=" * 60)
    logger.info("DIAGNOSTIC SUMMARY")
    logger.info("=" * 60)
    p50 = np.percentile(valid_probs, 50)
    p75 = np.percentile(valid_probs, 75)
    p90 = np.percentile(valid_probs, 90)
    p95 = np.percentile(valid_probs, 95)
    logger.info(f"  Percentiles: P50={p50:.4f}, P75={p75:.4f}, P90={p90:.4f}, P95={p95:.4f}")
    logger.info(f"  Pixels > 0.5: {(valid_probs > 0.5).sum()} ({(valid_probs > 0.5).mean()*100:.2f}%)")
    logger.info(f"  Pixels > 0.7: {(valid_probs > 0.7).sum()} ({(valid_probs > 0.7).mean()*100:.2f}%)")
    logger.info(f"  Pixels > 0.8: {(valid_probs > 0.8).sum()} ({(valid_probs > 0.8).mean()*100:.2f}%)")
    logger.info(f"  Pixels > 0.9: {(valid_probs > 0.9).sum()} ({(valid_probs > 0.9).mean()*100:.2f}%)")
    logger.info("=" * 60)
    
    # æ ¹æ®åˆ†å¸ƒç»™å‡ºå»ºè®®
    if p90 < 0.6:
        logger.warning("âš ï¸ æ¨¡å‹è¾“å‡ºæ¦‚ç‡æ™®éåä½ (P90 < 0.6)ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒæˆ–è°ƒæ•´æ¨¡å‹")
    elif p75 > 0.7:
        logger.info("âœ… æ¨¡å‹è¾“å‡ºæœ‰æ˜æ˜¾çš„é«˜ç½®ä¿¡åŒºåŸŸï¼Œé˜ˆå€¼ 0.7+ åº”èƒ½æœ‰æ•ˆåˆ†ç¦»")
    else:
        logger.info("ğŸ“Š æ¨¡å‹è¾“å‡ºä¸­ç­‰åä¸Šï¼Œå»ºè®®å°è¯• 0.6-0.75 çš„é˜ˆå€¼èŒƒå›´")


if __name__ == "__main__":
    main()
